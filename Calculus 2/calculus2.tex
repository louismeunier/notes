% backup|Calculus 2
\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Calculus 2}
\author{Notes by Louis Meunier}
\date{March 14 2021-August 7 2021}
\newcommand\longdiv[2]{%
$\strut#1$\kern.25em\smash{\raise.3ex\hbox{$\big)$}}$\mkern-8mu
        \overline{\enspace\strut#2}$}
\begin{document}

\maketitle

\section{Integration by Parts}
\subsection{The Rule}
Inverse of the product rule;

$(fg)' = f'g + fg'$

becomes;

$\int fg' + f'g = fg$

This format is understandable, but not very helpful. Subtracting one of the products of the integrals over to the opposite side creates a far more helpful form, typical written; $\int udv = uv - \int vdu$

\subsection{Examples}
\begin{enumerate}
    \item $\mathbf{\int xsin(x) dx}$
    
    $u = x; du = 1$
    
    $dv = sin(x); v = -cos(x)$
    
    $\int xsin(x) dx = x(-cos(x)) - \int(-cosx) * 1 dx$
    
    $= \mathbf{-xcos(x) + sin(x) + C}$
    \item $\mathbf{\int 5xe^{3x} dx}$
    
    $u = 5x; du = 5$
    
    $dv = e^{3x}; v = \frac{e^{3x}}{3}$
    
    $\int 5xe^{3x} dx = \frac{5xe^{3x}}{3} - 5\int\frac{e^{3x}}{3} dx$
    
    $= \mathbf{\frac{5xe^{3x}}{3} - \frac{5}{9}e^{3x} + C}$
\end{enumerate}

\subsection{Tips}
\begin{enumerate}
    \item Pick $u$ such that $du$ becomes simpler
    \item Pick $dv$ such that $v$ doesn't get more complicated
\end{enumerate}


\section{Trigonometric Identities}
Goal: to use trig identities to in substitution to evaluate integrals.
\subsection{List of Identities}
\begin{enumerate}
    \item $sin^{2}(x) + cos^{2}(x) = 1$
    \item $tan^{2}(x) + 1 = sec^{2}$
    \item $cot^{2}(x) + 1 = csc^{2}$
    \item $sin(2x) + 1 = 2sin(x)cos(x)$
    \item $cos(2x) = 2cos^{2}(x) - 1 = 1 - 2sin^{2}(x)$
    \item $sin^{2}(\frac{x}{2}) = \frac{1-cos(x)}{2}$
    \item $cos^{2}(\frac{x}{2}) = \frac{1+cos(x)}{2}$
\end{enumerate}
\subsection{Examples}
\begin{enumerate}
    \item $\mathbf{\int sin^{3}(x) dx}$
    
    $ = \int sin(x) sin^2(x) dx$
    
    $ = \int sin(x)(1-cos^2(x)) dx$
    
    $ [u = cos(x); du = -sin(x) dx]$
    
    $ = -\int (1-u^2) du$
    
    $ = -u + \frac{u^3}{3}$
    
    $ \mathbf{= cosx + \frac{cos^3}{3} + C}$
    
    \item $\mathbf{\int tan^3(x) sec^4(x) dy}$
    
    $ = \int u^3 sec^2(x) du$
    
    $ = \int u^3 (tan^2(x) +1) du$
    
    $ = \int u^3 (u^2+1) du$
    
    $ = \int u^5 + u^3 du$
    
    $...$
\end{enumerate}

\section{Trigonometric Substitution}
\subsection{Examples}
\begin{center}
\begin{enumerate}
    \item $\mathbf{\sqrt{1-x^2}dx}$, just the area of a circle
    
    $x = sin\theta, dx = cos\theta d\theta$
    
    $\int \sqrt{1 - sin^2\theta} cos\theta d\theta$ --- \textbf{trig identity}
    
    $ = \int \sqrt{cos^2\theta} cos\theta d\theta$
    
    $ = \int cos^2\theta d\theta$ --- \textbf{trig identity}
    
    $ = \int \frac{1 + cos(2\theta)}{2} d\theta  $ --- [$u = 2\theta, du = 2]$
    
    $ = \frac{\theta}{2} + \frac{sin(2\theta}{4} + C$
    
    $ \mathbf{= \frac{sin^{-1}x}{2} + \frac {sin(2sin^{-1}x)}{4} + C} $ --- \textbf{not quite done...}
\end{enumerate}
\end{center}
\subsection{Simplification}
\begin{center}
$sin(2sin^{-1}(x))$ --- $ [sin(2\theta) = 2sin(\theta)cos(\theta)]$

$ = 2sin(sin^{-1}(x))cos(sin^{-1}(x))$

$ \mathbf{= 2x \sqrt{1-x^2}}$
\end{center}
\subsection{Some good substitutions to remember}
\begin{center}
\begin{tabular}{ c c }
 \textbf{expression} & \textbf{substitution} \\
 $\sqrt{a^2-x^2}$ & $x = a sin(\theta)$ \\
 $\sqrt{a^2+x^2}$ & $x = a tan(\theta)$ \\
 $\sqrt{x^2-a^2}$ & $x = a sec(\theta)$ \\
\end{tabular}
\end{center}


\section{Partial Fractions}
\subsection{Theorem}
If $f(x) = \frac{P(x)}{Q(x)}$, where $P$ and $Q$ are polynomials and $deg(P) < deg(Q)$, the $f$ can be written as a sum of simple rational functions with either linear or quadratic denominators. 

\subsubsection{Example}
$\frac{x^3-12x^2+15}{x^7+19x^3+12} = \frac{A}{ax+b}+\frac{B}{cx+d} + ...$

Note: $deg(P) < deg(Q)$. If not, have to divide first.

\subsection{Basic Case}
If $f$ has only linear factors...

$f(x) = \frac{P(x)}{(a_1x + b_1)(a_2x + b_2)...(a_nx + b_n)} = \frac{A_1}{a_1x+b_1} + \frac{A_2}{a_2x+b_2} + ... + \frac{A_n}{a_nx+b_n}$

If some factors are repeated... 

$f(x) = \frac{P(x)}{(a_1x + b_1)^3(a_2x + b_2)^2} = \frac{A}{a_1x+b_1} + \frac{B}{(a_1x+b_1)^2} + \frac{C}{(a_1x+b_1)^3} + \frac{D}{a_2x+b_2} + \frac{E}{(a_2x+b_2)^2}$

\subsection{Irreducible Quadratic Factors}

If $f$ contains irreducible quadratic factors... (ex. $x^2+1$)

$f(x) = \frac{P(x)}{(x^2+1)^2} = \frac{Ax+B}{x^2+1} + \frac{Cx+D}{(x^2+1)^2}$

Any higher-degree polynomial can be reduced down to linear and quadratic factors.

\subsection{Examples}
\begin{enumerate}
    \item $\int \frac{x+3}{x^2-5x-6} dx$
    
    $\frac{x+3}{x^2-5x-6} = \frac{x+3}{(x-6)(x+1)} = \frac{A}{x-6} + \frac{B}{x+1}$
    
    $x+3 = Ax+A+Bx-6B$
    
    $x+3 = (A+3)x + A-6B$
    
    $A+B = 1, A-6B = 3$ (SOE)
    
    $A = 9/7, B = -2/7$
    
    $\int \frac{\frac{9}{7}}{x-6}+\frac{-\frac{2}{7}}{x+1}dx$
    
    $= \mathbf{\frac{9}{2} ln |x-6|-\frac{2}{7} ln |x+1| + C}$
    
    \item $\int \frac{x+1}{x^3-x^2}dx$
    $\frac{x+1}{x^3-x^2} = \frac{x+1}{x^2(x-1)} = \frac{A}{x} + \frac{B}{x^2} + \frac{C}{x-1}$
    
    $x+1 = Ax(x-1)+B(x-1)+Cx^2$
    
    At this point, we could solve for A, B, and C using a systems of equations. However, we can instead use helpful values of $x$ and solve more easily instead. 

    $x=0 => 1 = A*0 + B*-1 +C*0^2 => \mathbf{B = -1}$
    
    $x=1 => 2 = 0 + 0 + C => \mathbf{C = 2}$
    
    $x=2 => 3 = 2A + (-1) + 8 => \mathbf{A = -2}$

    Using this, we can now rewrite our original integral in a much nicer form.
    
    $\int \frac{-2}{x} + \frac{-1}{x^2} + \frac{2}{x+1} dx$
    
    $= \mathbf{-2\ln{x} + \frac{1}{x} + 2\ln{x+1} + C}$
    
    \item $\int \frac{10}{(x-1)(x^2+9)}dx$
    
    $\frac{10}{(x-1)(x^2+9)} = \frac{A}{x-1} + \frac{Bx+C}{x^2+9}$
    
    $10 = A(x^2+9) + (Bx+C)(x-1)$
    
    We can use the same idea as above; rather than solving using SOE, we can instead simply pick nice values for $x$
    
    $x=1 => 10 = A(10) => \mathbf{A=1}$
    
    $x=0 => 10 = 9A + C(-1) => \mathbf{C=-1}$
    
    $x=2 => \mathbf{B = -1}$
    
    Rewrite integral as...
    
    $\int \frac{1}{x-1} + \frac{-x-1}{x^2+9} dx$
    
    $= \int \frac{1}{x-1} + \frac{-x}{x^2+9} - \frac{1}{x^2+9} dx$
    
    First factor uses basic integration, second uses $u$ substitution, and the last uses the fact that $\int \frac{1}{x^2+a^2} dx= \frac{1}{a}tan^{-1}(\frac{x}{a})$
    
    $= \mathbf{\ln |x-1| - \frac{1}{2} \ln |x^2+9| - \frac{1}{3}tan^{-1}(\frac{x}{3}) + C}$
    
    \item $\int \frac{x^3+4}{x^2+4} dx$
    
    Since $deg(P) > deg(Q)$, we have to do some work first by dividing $P$ by $Q$.
    
    $(x^3+4)/(x^2+4) = x + \frac{-4x+4}{x^2+4}$
    
    $\int x + \frac{-4x+4}{x^2+4} dx$
    
    This way, we already have a irreducible quadratic factor, so we can integrate normally.
    
    $ = \mathbf{\frac{x^2}{2} -2\ln |x^2+4|+2tan^{-1}(\frac{x}{2}) + C}$
    
    \item $\int \frac{dx}{2\sqrt{x+3}+x}$
    
    In this example, we don't even have a rational function. We need to transform it first through substitution.
    
    $u = \sqrt{x+3}$
    
    $x = u^2-3, 2udu = dx$
    
    $\int \frac{2u du}{(u+3)(u-1)} $
    
    Work as before...
    
    $= \frac{3}{2}\ln(\sqrt{x+3}+3) + \frac{1}{2}\ln(\sqrt{x+3}-1) + C$
\end{enumerate}

\section{Integration Strategies}
How do I know what technique to use?

\subsection{Simplifying}
A helpful way to think about all this is to simplify: in reality, we only have 2 integration techniques, with other techniques just subsets/algebraic manipulation.
\begin{enumerate}
    \item Substitution
    
    \item Integration by Parts
\end{enumerate}

So which one to use?
\begin{enumerate}
    \item \textbf{Try u-substitution first (with simplification). Did it work?}
    \item \textbf{If not, try integration by parts (again with simplification). Did it work?}
    \item \textbf{If not, think of other algebraic tricks to manipulate the problem}
    \begin{enumerate}
        \item rational function $\rightarrow$ partial fractions
        
        \item trig $\rightarrow$ trig identities
        
        \item $\sqrt{a^2 \pm x^2}$-type $\rightarrow$ trig subs, trig identities
    \end{enumerate}
\end{enumerate}

\section{Integration Tables}
Tables found in most calculus textbooks, of many common integrals with the work shown. Essentially obsolete nowadays.

\section{Approximate Integration}

\subsection{An Example Demonstrating Need}

ex) $\int_{0}^{5} e^{x^{2}} dx$

$ = F(5)-F(0)$ (normally, with FTC)

However, $e^{x^{2}}$ does not have an anti-derivative that we can write with elementary functions.

Instead, we have to approximate the value of this (using, say, Riemann sums).

\subsection{Riemann Approximation}
Approximate
$\int_{0}^{4} \sqrt{x} dx$ 
using right-hand rule w/ intervals.

$\Delta x(\sqrt{1} + \sqrt{2} + \sqrt{3} + \sqrt{4}) \mathbf{\approx 6.146}$

\subsection{Midpoint and Trapezoidal Rules}

$\int_{0}^{4} \sqrt{x} dx$ 

\subsubsection{Midpoint}

Same as Riemann, but using the midpoint of each rectangle approximation.

$\Delta x (\sqrt{\frac{1}{2}}+  \sqrt{\frac{3}{2}}+ \sqrt{\frac{5}{2}}+ \sqrt{\frac{7}{2}}) \mathbf{\approx 5.384}$

\subsubsection{Trapezoidal}

Same as Riemann, but adjusting the "top" side to form more approximate trapezoids.

$\Delta x (\frac{\sqrt{0} + 2\sqrt{1} + 2\sqrt{3} + \sqrt{4}}{2}) \mathbf{\approx 5.146}$

\subsubsection{General Formulas}
\begin{enumerate}
    \item \textbf{Midpoint Rule}
    
    $\int_{a}^{b} f(x) \,dx \approx \sum_{i=0}^{n-1} f(\frac{x_i+x_{i+1}}{2}) \Delta x $
    
    \item \textbf{Trapezoidal Rule}
    
    $\int_{a}^{b} f(x) \,dx \approx \frac{1}{2}\Delta x f(x_0) + 2f(x_1) + ... + 2f(x_{n-1}) + f(x_n)$ where $\Delta x = \frac{b-a}{n}$
\end{enumerate}

\subsection{Simpson's Rule}

\begin{enumerate}
    \item Divide the function into an even number of points
    \item Use every 3 points to create a number of parabolas, imitating the curve
\end{enumerate}

\subsubsection{Formula}
$\int_{a}^{b} f(x) \,dx \approx \frac{1}{3} \Delta x(f(x_0) + 4f(x_1) + 2f(x_2) + ... + 2f(x_{n-2}) + 4f(x_{n-1}) + f(x_n))$ where $\Delta x = \frac{b-a}{n}$ and $n$ is an even number.

ex) $\int_{0}^{4} \sqrt{x} \,dx = \frac{1}{3}(1)(\sqrt{0} + 4\sqrt{1} + 2\sqrt{2} + 4\sqrt{3} + \sqrt{4} \approx 5.25)$

\subsection{Error Bounding}

\subsubsection{What makes an approximation bad?}
In general, our rules for approximation assume a nice, smooth, not-too-crazy curve. Very curvy (second and higher derivatives are large) functions cause worse approximations. 

Things that matter in approximations: 
\begin{enumerate}
    \item Higher order derivatives
    \item The number of intervals
    \item Length of interval of integration ($b-a$)
\end{enumerate}

\subsubsection{Error Bounds for Trapezoid and Midpoint Approximation}

If $E_T$ and $E_M$ are the errors in the trapezoidal and midpoint rules, respectively, and if $f''(x) \leq K$ for all $x$ on $[a,b]$, then 

\begin{center}
    $|E_T| \leq \frac{K(b-a)^3}{12n^2}$ and $|E_M| \leq \frac{K(b-a)^3}{24n^2}$
\end{center}

where $n$ is the number of intervals of integration.
\newline
ex) Suppose we approximate $\int_{2}^{6}e^{-2x} \,dx$ using the midpoint with $n=5$. What is the largest the error could be?

$f'(x) = -2e^{-2x}, f''(x) = 4e^{-2x}$

$|f''(x)|\leq 4e^{-4} = \frac{4}{e^4}$

$|E_M| \leq \frac{\frac{4}{e^4}(6-2)^3}{24(5)^2}$

\subsubsection{Error Bounds for Simpson's Rule}
If $E_S$ is the error in Simpson's Rule and if $f^{(4)}(x) \leq K$ for all $x$ on $[a,b]$, then 
\begin{center}
    $|E_S| \leq \frac{K(b-a)^5}{180n^4}$
\end{center}

ex) Suppose we want to approximate $\int_{0}^{1} sin(2x) \,dx$ to within a maximum error of $.000002$. How large do we need to let $n$ be? (Using Simpson's).

$f'(x) = 2cos(2x)$ 

$f''(x) = -4sin(2x)$

$f^{(3)}(x) = -8cos(2x)$

$f^{(4)}(x) = 16sin(2x)$

Find $n$...

$0.000002 \leq \frac{16(1)^5}{180n^4}$

$n > 14.5$

We need an even number of intervals for Simpson's, therefore $n$
 must be at least 16.
 
\subsection{Improper Integrals}

\subsubsection{A Use Case}

e) What is the area between the graph of $\frac{1}{x^2}$ and the x-axis?

Can't just use a standard integral: the graph of $\frac{1}{x^2}$ continues forever to the right. Therefore, we have to deal with infinity in some way... Need to use a limit!

\subsubsection{Definition}
If $\int_{a}^{b} f(x) \,dx$ exists for all $b$, we can define 

\begin{center}
    $\int_{a}^{\infty} f(x) \,dx = \lim_{b\to\infty} \int_{a}^{b} f(x) \,dx$.
\end{center}

If the limit does not exist, we say the improper integral is \textit{divergent}. If the limit does exist, it is \textit{convergent}.

Similarly, we define

\begin{center}
    $\int_{-\infty}^{b} f(x) \,dx = \lim_{a\to-\infty} \int_{a}^{b} f(x) \,dx$.
\end{center}

\subsubsection{Examples}

ex) What is the area between the graph fo $\frac{1}{x}$, the line $x=1$, and the x-axis.

\begin{center}
    $\int_{1}^{\infty} \frac{1}{x} \,dx = \lim_{b\to\infty} \int_{1}^{b} \frac{1}{x} \,dx$
    
    $=\lim_{b\to\infty} \ln{|x|} ] 1,b$
    
    $=\lim_{b\to\infty} \ln{b}-\ln{1}$
    
    $=\infty$
\end{center}

Therefore, this interval is divergent.

For negative to positive infinity intervals...

\begin{center}
    $\int_{-\infty}^{\infty} f(x) \,dx = $
    
    $\int_{a}^{\infty} f(x) \,dx + \int_{-\infty}^{a} f(x) \,dx$
\end{center}

for any value $a$.

ex) $\int_{-\infty}{\infty} \frac{x^2}{9+x^6} \,dx$

\begin{center}
    $\lim{a\to-\infty} \int_{a}^{0} \frac{x^2}{9+x^6} \,dx + \lim_{b\to\infty} \int_{0}^{b} \frac{x^2}{9+x^6} \,dx$
    
    $=\lim_{a\to-\infty} \frac{1}{9} tan^{-1}(\frac{x^3}{3})|[a,0] +\lim_{b\to\infty} \frac{1}{9} tan^{-1}(\frac{x^3}{3})|[0,b] $
    
    $=0-\frac{1}{9}(\frac{-\pi}{2}) + \frac{1}{9}(\frac{\pi}{2}) - 0$
    
    $=\mathbf{\frac{pi}{9}}$
\end{center}

Non-infinite answer, therefore, the interval is convergent.

\subsubsection{More Impropriety}

ex) $\int_{0}^{4} \frac{1}{x-1} \,dx$

This is improper because it has an asymptote at $x=1$.

$\int_{0}^{1} \frac{1}{x-1} \,dx + \int_{1}^{4} \frac{1}{x-1} \,dx$

$= \lim_{b\to1-}\int_{0}^{b} \frac{1}{x-1} \,dx + \lim_{a\to1+} \int_{a}^{4} \frac{1}{x-1} \,dx$

$=\lim_{b\to1-} \ln{|x-1|} |[0,b] ...$

$=\mathbf{-\infty}$

Therefore, the integral is divergent.

\subsubsection{A Comparison Theorem}

Can be used to test for convergence or divergence of an improper integral; cannot find the value of the integral.

\begin{center}
    If $0 \leq g(x) \leq f(x)$ for all $x \geq a$,
    
    Then 
    
    $\int_{a}^{\infty} f(x) \,dx $ converges $\rightarrow \int_{a}^{\infty} g(x) \,dx$ is convergent.
    
    $\int_{a}^{\infty} g(x) \,dx $ diverges $\rightarrow \int_{a}^{\infty} f(x) \,dx$ is divergent.
    
\end{center}

ex) Show that $\int_{1}^{\infty} e^{-x^2} \,dx$ is convergent.

If $x \geq 1$, then $e^{-x^2} \leq e^{-x}$

$\int_{1}^{\infty} e^{-x} \,dx = \lim_{b\to\infty} -e^{-x} | [1,b]$

$=\lim_{b\to\infty} -e^{-b} + e^{-1}$

$= \frac{1}{e}$

This is convergent, and less than the original, therefore the original is also convergent.

\section{Arc Length}

\subsection{Formula}

$\int_{a}^{b} \sqrt{1+(f'(x))^2} \,dx$

\subsection{Examples}

\begin{enumerate}
    
\item What is the distance traveled by a particle traveling along the $x^3 = y^2$ from $(0,0)$ to $(4,8)$.

$y = x^{3/2}$

$y' = \frac{3}{2}x^{\frac{1}{2}}$

Arc Length = $\int_{0}^{4} \sqrt{1+(\frac{3}{2}\sqrt{x})^2} \,dx$

... = $\frac{8}{27}(10^{\frac{3}{2}}-1)$

\item What is the arc length of the curve $y = x^{\frac{1}{3}}$ from $-1$ to $1$?

$\int_{-1}^{1} \sqrt{1+ (\frac{1}{3x^{\frac{2}{3}}})^2} \,dx$

This is an improper integral! We could continue integrating this, but instead we could integrate with respect to $y$ to create a nicer problem.

$x = y^3$

$\int_{-1}^{1} \sqrt{1+(3y^2)^2} \,dy$
\end{enumerate}

\subsection{Arc Length Function}

If $y=f(x)$, $S(x) = \int_{a}^{x} \sqrt{1+(f'(t))^2} \,dt$  is the arc length from a to x.

$\frac{ds}{dx} = \sqrt{1+(\frac{dy}{dx})^2}$

$ds = dx \sqrt{1+\frac{dy^2}{dx^2}}$

$ds^2 = dx^2 + dy^2$

This is really just the Pythagorean theorem!

\section{(Lateral) Area of a Surface of Revolution}

\subsection{Surface Area of a Frustum}

How do we calculate the area of this not-quite-a-cylinder?

We could compute it as a difference of 2 cones.

From here, we can derive the formula $SA = 2\pi r l$

\subsection{Area of a Surface of Revolution}
\begin{center}
    
$SA = \sum 2 \pi r l$

$ = \sum 2 \pi (\frac{f(x_i)+f(x_{i+1}}{2}) \sqrt{1+f'(c_i)^2} \Delta x$

As $\Delta x \rightarrow 0 f(x_{i+1} \approx f(c_i)$, so

$ = \sum 2\pi f(c_i) \sqrt{1+(f'(c))^2} \Delta x$

As $n \rightarrow \infty, \Delta x \rightarrow 0$

$ = 2\pi \int f(x) \sqrt{1+(f'(x))^2} \,dx$

\end{center}

This is our surface area.

\subsection{Example}


\begin{enumerate}
    \item What is the surface area of a sphere of radius ${r}$?
    
    \begin{center}
    $y = \sqrt{r^2 - x^2}$
    
    $SA = 2\pi \int_{-r}^{r}
    \sqrt{r^2-x^2}\sqrt{1+(\frac{-2x}{2\sqrt{r^2-x^2})}} \,dx$
    
    $ = 2\pi \int_{-r}^{r} \sqrt{r^2-x^2} \sqrt{\frac{r^2}{r^2-x^2}} \,dx$
    
    $ = 2\pi \int_{-r}^{r} r dx$
    
    $ = 4\pi r^2$
    
    This matches our known formula
    \end{center}
    
    \item What is the area of the surface of revolution obtained by rotating the curve $y=x^{1/3}$, from $1 \leq y \leq 2$, around the $y$-axis?
    
    \begin{center}
        $SA = 2 \pi \int_{1}^{2} x \sqrt{1 + (\frac{dx}{dy})^2} \,dy$
        
        $ = 2 \pi \int_{1}^{2} y^3 \sqrt{1+ (3y^2)^2} dy$
        
        $ = \pi/27(145\sqrt{145} - 10\sqrt{10})$
    \end{center}
\end{enumerate}


\section{Sequence}

\subsection{Definition}

An infinite list of numbers, in order, indexed by natural numbers.

\subsection{Expression Sequences}

Common notation: $\{a_1,a_2,a_3, ...\} = \{a_n\} = \{a_n\}_{n=1}^{\infty}$

ex) $\{3/n\} = \{3/1,3/2,3/3,3/4...\}$

ex) $\{0,3,8,15,24,...\} = \{a_n\}$ where $a_n = n^2-1$


Not all sequences can be expressed via formula, however, ex) $\{a_b\}$ where $a_n=nth$ digit of $\pi$, or $\{F_n\}$ where $F_1=1$, $F_2=1$, $F_{n+1} = F_n+F_{n-1}$ (Fibonacci sequence).

These are a recursive formula. These can sometimes, but not always, turned into an explicit formula. 

\subsection{Sequence Terminology}

\begin{enumerate}
    \item A sequence is \textit{increasing} if $a_n < a_{n+1}$ for all $n\geq1$.
    \item A sequence is \textit{decreasing} if $a_n > a_{n+1}$ for all $n\geq1$.
    \item A sequence is \textit{monotonic} if it is increasing or decreasing.
    \item A sequence is \textit{bounded above} if there exists some $M$ such that $a_n\leq M$ for all $n\geq1$.
    \item A sequence is \textit{bounded below} if there exists some $M$ such that $a_n\geq M$ for all $n\geq1$.
    \item A sequence is \textit{bounded} if it is bounded from above and below
\end{enumerate}

\subsection{Limits of a Sequence}

\subsubsection{Definition}

$\lim_{n\to\infty} a_n = L$ if for all $\epsilon > 0$ there is some $N$ such that if $n<N$, then $|a_n-L|<\epsilon$. 

If this limit exists (ie, closes to a point), then the function converges. Otherwise, the function diverges (ie, fluctuates between two points, continues to infinity).

\subsubsection{A Theorem}

If $a_n$ is given by a function $f$ ($a_n=f(n)$), and $\lim_{x \to \infty} f(x)=L$, then $\lim_{n\to\infty}a_n=L$.

So all the limit laws we have for functions will work for sequences as well. 

\subsubsection{Examples}

ex) Does $\{\frac{(-1)^n}{\sqrt{n}}\}$ converge?

$f(x) = \frac{(-1)^x}{\sqrt{x}}$

Apply Squeeze Theorem:
$\frac{-1}{s\sqrt{n}}<\frac{(-1)^x}{\sqrt{x}} < \frac{1}{s\sqrt{x}}$

$\lim_{x\to\infty} \frac{1}{2\sqrt{x}} = 0$

$\lim_{x\to\infty} -\frac{1}{2\sqrt{x}} = 0$

Therefore, $\lim_{x\to\infty}\frac{(-1)^x}{2\sqrt{x}} = 0$

ex) Does the sequence $\{a_n\}$ where $a_1=3$ and $a_{n+1}=\frac{a_n}{n}$ converge?

Each term multiplies the previous by $1/n$.

$a_n = \frac{3}{1*2*3*...(n-1)} = \frac{3}{(n+1)!}$'

$\lim_{n\to\infty}a_n$ = 0

ex) For what values of $r$ does $\{r^n\}$ converge?

$f(x) = r^x$

If $r>1$, exponential growth. If $0<r<1$, exponential decay. In exponential decay, $f(x)$ is heading towards a limit, and therefore it converges. 

But what if $r<0$?

If $-1<r<0$, then by Squeeze Theorem, converges to 0.

If $r<=-1$, diverges.

In summary, converges if $-1<r<=1$. If $01<r<1$, converges to 0. If $r=1$, converges to 1.

\subsubsection{Another Theorem}

Every bounded, monotonic sequence is convergent.

Can't go in one direction infinitely since it is bounded; similarly, can't go another direction since it is monotonic.

\section{Series}
\subsection{Zeno's Paradox}

Someone is walking towards a wall. He, of course, at some point walks halfway, leaving half the total distance remaining to walk. Then, he walks half of the remaining, then again and again, etc., always having a little left to reach the wall.

So what's going on? Does $1/2+1/4+1/8+...$ equal 1?

\subsection{Decimal Representations}

$\pi \approx 3.14159 = 3 + 1x10^{-1}+4x10^{-1}+1x10^{-3}+5x10^{-4}+...$

These kinds of irrational decimals are in fact infinite sums. 

\subsection{Power Series}

Claim:
$\frac{1}{1-x} = 1+x+x^2+x^3+...$

$1=(1+x+x^2+...)(1-x)$

$1 = 1(1-x)+x(1-x)+x^2(1-x)+...$

$1 = 1-x+x-x^2+x^2-x^3+x^3+...$

$1=1$

This like a true statement, and demonstrates a way to represent a fraction as an infinite series.

\subsection{Issues}

$0 = 0+0+0+0+...$

$ = (1-1)+(1-1)+(1-1)+...$

$=1+(-1+1)+(-1+1)+(-1+1)+...$

$=1+0...$

$=1$

Right? Well, we can't just use series like this all the time; there are some caveats.

\subsection{Defining Infinite Series}

$\sum_{i=1}^{\infty} a_i = \lim_{n\to\infty} \sum_{i=1}^{n} a_i$ 

The last summation is often written as $s_n$.

If the limit does not exist, then the sum is divergent. 

\subsubsection{Example}

ex) $\sum_{n=1}^{\infty} 3n+2$

$ = (3+2)+(6+2)+(9+2)+(12+2)+...$

$5+8+11+14+...$

Partial sums: $5,13,24,38,...$

This is divergent: do not approach any finite number.


ex) $\sum_{n=1}^{\infty}(\frac{1}{2})^n$

$=1/2+1/4+1/8+1/16+...$

$s_1 = 1/2$

$s_2 = 3/4$

$s_3 = 7/8$

...

It appears that $s_n = \frac{2^n-1}{2^n}$, meaning that $\lim_{n\to\infty} \frac{2^n-1}{2^n} = \lim_{n\to\infty} 1-\frac{1}{2^n} = 1$.

ex) This is the harmonic series: $\sum_{n=1}^{\infty}\frac{1}{n}$

$ = 1+1/2+1/3+1/4+1/5$

Converge or diverge?

This actually diverges; although the partial sums are getting smaller, there is no real pattern, and this limit equals $\infty$.


\subsection{Theorems}

\begin{enumerate}
    \item If $\sum a_n$ converges, then $\lim_{n\to\infty} a_n = 0$.

This is often used in the divergence test: If $\lim_{n\to\infty} \neq 0$, then $\sum a_n$ diverges.

Note: Converse of this statement is not true; $\lim_{n\to\infty} a_n = 0$ does not necessarily mean that the sum converges; the harmonic series falls into this category.

\item Since series are defined via limits, the usual limit laws hold. 

If $\sum a_n$ and $\sum b_n$ both converge, then
\begin{enumerate}
    \item $\sum ca_n = c\sum a_n$
    \item $\sum (a_n+b_n) = \sum a_n + \sum b_n$
    \item $\sum (a_n-b_n) = \sum a_n - \sum b_n$
\end{enumerate}

However, if either sum is divergent, then these rules will not always hold.
\end{enumerate}

\subsection{General Examples}
\begin{enumerate}
    \item Write down the first four partial sums of $\sum_{n=1}^{\infty} \frac{(-1)^n n}{2^n}$.
    
    = $-1/2 + 2/4 + -3/8 + 4/16+...$
    
    $s_1 = -1/2$
    
    $s_2=0$
    
    $s_3 = -3/8$
    
    $s_4 = -1/8$
    
    
    \item What is $\sum_{n=1}^{\infty}r^{n-1}$?
    
    This is an example of a geometric series.
    
    $s_n = 1+r+...+r^{n-1}$
    
    $rs_n = r+r^2+...+r^n$
    
    $s_n-rs_n = 1-r^n$
    
    $s_n = \frac{1-r^n}{1-r}$
    
    $\lim_{n\to\infty}s_n = \lim_{n\to\infty}\frac{1-r^n}{1-r} = \frac{1}{1-r}$ if $|r| < 1$. Otherwise, diverges.
    
    \item What is $\sum_{n=1}{\infty} \frac{(-4)^{n+1}}{4^n}$?
    
    $=9/4 + -27/16+...$
    
    Rewrite:
    
    $=\sum \frac{(-3)^n(-3^n-1)}{4(4)^{n-1}}$
    
    $=\frac{9}{4}\sum_{n=1}^{\infty} (-\frac{3}{4})^{n-1}$
    
    This is a geometric series.
    
    $=\frac{9}{4}(\frac{1}{1-(-\frac{3}{4})})$
    
    $=\frac{9}{4}*\frac{1}{\frac{7}{4}}$
    
    $=\frac{9}{7}$
    
    \item Write $0.888...$ as a ratio of two integers.
    
    $ = 8/10 + 8/100 + 8/1000 + ...$
    
    $ = 8x10^{-1} + 8x10^{-2} + 8x10^{-3} + ...$
    
    $ = \sum_{n=1}^{\infty} 8(\frac{1}{10})^n$
    
    $= \sum_{n=1}^{\infty} \frac{8}{10}(\frac{1}{10})^{n-1}$
    
    $=\frac{8}{10} * \frac{1}{1-\frac{1}{10}}$
    
    $=\frac{8}{9}$
    
    \item True or false: if two series, $\sum a_n$ and $\sum b_n$ both diverge, then $\sum(a_n+b_n)$ also diverges.
    
    False; take:
    
    $\sum a_n = \sum 1$, $\sum b_n + \sum -1$
    
    $\sum(1+-1)$
    
    $ = 0$
    
    \item True or false: if $\sum a_n$ diverges and $\sum b_n$ converges, then $\sum (a_n+b_n)$ diverges.
    
    True; $\sum b_n$ is a single number. $sum a_n$ either goes to infinity, or something similar. Therefore, adding the two together still results in some sort of "infinity".
    
    A more rigorous proof by contradiction:
    
    Suppose it was false; $\sum a_n$ diverges, $\sum b_n$ converges, and $\sum (a_n+b_n)$ converges.
    
    Then: $\sum (a_n+b_n)-\sum(b_n)$
    
    $=\sum (a_b +b_n-b_n)$
    
    $=\sum a_n$, which should converge, but doesn't...
    
\end{enumerate}

\section{The Integral Test}

\subsection{Why}
Sometimes it is very hard to compute the limit of a series.

Can we at least tell if it converges or diverges?

\subsection{Picturing Partial Sums}

Suppose $a_n = f(n)$ for some function $f$.

$\sum a_n = 2+3+2+1+1/2$

This can be drawn as a number of rectangles of different heights of width 1. The sum is just the sum of the areas; very similar to Riemann sums.

\subsubsection{Examples}

\begin{enumerate}
    \item $a_n=\frac{1}{n^2}$

$\sum a_n = 1+1/4+1/9+1/16+...$

$\sum a_n \leq \int_1^{\infty} \frac{1}{x^2} \,dx +1 $

    \item $a_n = \frac{1}{\sqrt{n}}$
    
    $\sum a_n \geq \int_1^{\infty} \frac{1}{\sqrt{x}} \,dx$
    

\end{enumerate}

\subsection{The Test}

Suppose $a_n=f(n)$, where $f$ is a continuous, positive, decreasing function on $[1,\infty\}$. Then the series $\sum a_n$ is convergent if and only if the integral $\int_1^{\infty} f(x) \,dx$ is convergent.


\subsection{Remainders}

$\sum a_n \lim_{n\to\infty} s_n$, where $s_n=a_1+a_2+...$

Compare $s_n$ with $\sum a_n$

$\int_{n+1}^{\infty} f(x) dx \leq R_n \leq \int_{n}^{\infty} f(x) dx $

The error between the $n$th partial sum $s_n$ and $s$ can be bounded by the above.

$s_n+R_n=s$

$s_n + \int_{n+1}^{\infty} f(x) dx \leq s \leq s_n+\int_{n}^{\infty} f(x) dx $


\subsection{Examples of the Integral Test}

\begin{enumerate}
    \item $\sum_{n=1}^{\infty} \frac{n}{n^2+1}$
    
    $\int_1^{\infty} \frac{x}{x^2+1} dx = \lim_{b\to\infty} \int_1^b\frac{x}{x^2+1} dx = \lim_{b\to\infty} 1/2 \ln{x^2+1} |_1^b$
    
    
    $=\lim_{b\to\infty} 1/2\ln{b^2+1} - 1/2\ln{2}$
    
    $=\infty$
    
    The series diverges.
    
    \item $\sum_{n=1}^{\infty} \frac{1}{n^3}$
    
    $\int_1^{\infty} \frac{1}{x^3} dx = \lim_{b\to\infty} -\frac{1}{2x^2} |_1^b$
    
    $=\lim_{b\to\infty} -\frac{1}{2b^2}+1/2$
    
    $ = 1/2$
    
    Therefore, the series converges (though not to 1/2...)
    
    Furthermore...
    
    $\int_{n+1}^\infty \frac{1}{x^3} dx \leq R_n \leq \int_{n}^\infty \frac{1}{x^3} dx$
    
    $\frac{1}{2(n+1)^2} \leq R_n \leq \frac{1}{2n^2}$
    
    If we wanted to be within $a$, we need $R_n \leq a$. If $\frac{1}{2n^2}\leq a$, this works.
    
    \item \textbf{p-series}
    
    Series of form $\sum \frac{1}{n^p}$.
    
    For what values of $p$ does a p-series converge?
    
    If $p\leq0$, clearly diverges.
    
    Otherwise, $\int_1^\infty \frac{1}{x^p} = \lim_{b\to\infty} -\frac{1}{px^{p-1}} |_1^b$
    
    $ = \lim_{b\to\infty} -\frac{1}{pb^{p-1}} + 1/p$
    
    This converges if $p>1$. 
    
    Note: if $p=1$, it is the harmonic series. 
    
    p-series converges if $p>1$.
    
\end{enumerate}


\section{The Comparison Test}

\begin{enumerate}
    \item If $b_n \geq a_n$ and $\sum b_n$ converges, then $\sum a_n$ converges.
    \item If $b_n \leq a_n$ and $\sum b_n$ diverges, then $\sum a_n$ diverges.
\end{enumerate}


\subsection{Examples}

\begin{enumerate}
    \item $\sum_5^\infty \frac{3}{n-4}$
    
    $=3\sum \frac{1}{n-4}$ 
    
    Very close to $\sum 1/n$; we can compare to it.
    
    $\frac{1}{n-4} \geq \frac{1}{n}$
    
    Since $\sum 1/n$ diverges, so does our sum.
    
    \item $\sum_{n=3}^\infty \frac{n-1}{(n+1)^3}$
    
    $\frac{n-1}{(n+1)^3} \leq \frac{n+1}{(n+1)^3} = \frac{1}{(n+1)^2} \leq \frac{1}{n^2}$
    
    $\sum \frac{1}{n^2}$ converges, therefore our series converges as well.
    
    \item $\sum \frac{\sqrt{n^4-1}}{n^3+n}$
    
    This is kind of like $1/n$; $\sqrt{n^4-1} \approx \sqrt{n^4} = n^2$ and $n^3+n \approx n^3$; $\approx \frac{n^2}{n^3}$
    
    This is hard to prove algebraically however; this is where we use Limit Comparison.
\end{enumerate}

\subsection{Limit Comparison}

Suppose that $\sum a_n$ and $\sum b_n$ are series with positive terms. If $\lim_{n\to\infty} \frac{a^n}{b^n} = c$, where $0<c<\infty$, then either both series converge or both series diverge.

Ex) $\sum \frac{\sqrt{n^4-1}}{n^3+n}$

$(\lim_{n\to\infty} \frac{\sqrt{n^4-1}}{n^3+n})/\frac{1}{n}= \lim_{n\to\infty} \frac{\sqrt{n^4+1}/\sqrt{n^4}}{1+\frac{1}{n^2}} = 1$

Since this series diverges, then so does our original series.


\section{Alternating Series}

A series whose terms alternate between positive and negative.

$\sum a_n = \sum (-1)^{n-1}b_n or \sum (-1)^n b_n$, $b_n\geq 0$

\subsection{Alternating Series Test}

If the series $\sum_{n=1}^\infty (-1)^n b_n$ satisfies:
\begin{enumerate}
    \item $b_{n+1}\leq b_n$ for all $n$
    \item $\lim_{n\to\infty} b_n = 0$
\end{enumerate}

then the series converges.

ex) $\sum \frac{(-1)^{n-1}}{n}$

$1/n \geq \frac{1}{n+1}$

$\lim_{n\to\infty} 1/n = 0$

Therefore, it converges.

\subsection{Error}

$|R_n| \leq b_{n+1}$

\section{Comparison Test}

\subsection{Basics}

If $b_n \geq a_n$ and $\int b_n$ converges, then $\int a_b$ converges.

If $b_n \leq a_n$ and $\int b_n$ diverges, then $\int a_b$ diverges.

\subsection{Limit Comparison}

Sometimes, comparing whether a sum is larger or smaller than another is too complex. In these cases we can use: 

If $\lim_{x\to\infty} \frac{a_n}{b_n} = c$, where $c$ is a finite number, then $a_n$ and $b_n$ are both divergent or convergent. 

\section{Alternating Series}

Very simple: if an alternating series (switches between positive and negative): 

\begin{enumerate}
    \item $b_n < b_{n+1}$ (ie, do consecutive elements (ignoring alternating signs) decrease for all $n$?)
    \item $\lim_{n\to\infty} b_n = 0$
\end{enumerate}

\section{Absolute vs. Conditional Convergence}

A series $\sum a_n$ is absolutely convergent if $\sum |a_n|$ converges.

A series $\sum a_n$ is conditionally convergent if $\sum |a_n|$ diverges but $\sum a_n$ converges. (ex. alternating harmonic series)

\subsection{Example}
$\sum_{n=0}^{\infty} \frac{(-1)^n}{3n+5}$

Does it converge? \textbf{yes} (alternating series test)

Does $\sum |a_n|$ converge? \textbf{no} (limit comparison test)

Therefore, the sum is conditionally convergent.

\section{Ratio Test}

Given a series $\sum a_n$: 

\begin{enumerate}
    \item if $\lim |\frac{a_{n+1}}{a_n}| = L < 1$, then the series converges absolutely 
    \item if $\lim |\frac{a_{n+1}}{a_n}| = L > 1$, then the series diverges
    \item if $\lim |\frac{a_{n+1}}{a_n}| = L < 1$, then the Ratio test is inconclusive
\end{enumerate}

\section{Root Test}

Given a series $\sum a_n$: 

\begin{enumerate}
    \item if $\lim nth root of |a_n| = L < 1$, then the series converges absolutely 
    \item if $\lim nth root of |a_n| = L > 1$, then the series diverges
    \item if $\lim nth root of |a_n| = L = 1$, then the test is inconclusive
\end{enumerate}

\section{Power Series}

A series of the form:

$\sum_{n=0}^{\infty} c_nx^n = c_0 + c_1x + c_2x^2 + c_3x^3 + ...$

where the $c_n$ are coefficients and $x$ is a variable.

These are like polynomials, but with an infinite number of terms. 

\subsection{Examples}

What does it mean to converge? 

    For a fixed value of $x$, does the series converge?
    
    What are all the values of $x$ for which the series converges?
    
\begin{enumerate}
    \item $\sum_{n=0}^{\infty} x^n = 1 + x + x^2 + x^3 + x^4 + ...$
    
    This example is a geometric series, so it converges if $-1<x<1$.

    \item $\sum_{n=0}^{\infty}\frac{x^n}{n!} = 1 + x + x^2/2 + x^3/6 + x^4/24 + ...$

    We can use the ratio test to find convergence.
    
    $\lim_{n\to\infty} \frac{\frac{x^{n+1}}{(n+1)!}}{\frac{x^n}{n!}}$
    $=\lim_{n\to\infty} \frac{x^{n+1}}{(n+1)!}*\frac{n!}{x^n} $
    $\lim_{x\to\infty}\frac{x}{n+1}$
    $= 0$ for any fixed $x$.
    
    Therefore, this converges for all values of $x$
\end{enumerate}

\subsection{Power Series at $a$}

A power series of the form 

$\sum c_n(x-a)^n$

is called a \textit{power series about $a$}.

All the series above have been series about 0. 

Note: if $x=a$, then the series will converge. when $x$ is "near" $a$, $x-a$ will be "small", so more likely to converge.

\subsubsection{Radius of Convergence}

Give a power series about $a$, there are only three possibilities: 

\begin{enumerate}
    \item The series converges at $x=0$
    \item The series converges for all values of $x$
    \item There is a number $R$ such that the series converges if $x$ is within $R$ of $a$, and the series diverges if $x$ is $R$ away from $a$. R is also known as the "radius of convergence".
\end{enumerate}

\subsubsection{Examples}

Find the interval and radius of convergence of 

$\sum_{n=1}^{\infty} 4^n(x-1)^n$

We can use the root test here: 

$\lim_{n\to\infty} nth root of 4^n(x-1)^n = \lim_{n\to\infty} 4(x-1)$

Therefore, converges if $|4(x-1)| < 1$, ie, if $x$ is in the interval $(3/4, 5/4)$.

It diverges if $|4(x-1)|>1$, ie, if $x$ is in $(-\infty,3/4)U(5/4,\infty)$

What if $x=3/4,5/4$? 

    $x=3/4 => \sum 4^n(-1/4)^n = \sum (-1)^n => $ diverges
    
    $x=5/4 => \sum 4^n(1/4^n) = \sum 1^n => $ diverges.
    
    The radius of convergence is therefore $1/4$, and the interval of convergence is $(3/4, 5/4)$.
    
\section{Functions as Power Series}
$\frac{1}{1-x} = \sum_{n=0}^{\infty} x^n$
\subsection{Examples}

\begin{enumerate}
    \item ex) $\frac{1}{1+x}$
    
    First way to generate new power series is to use our old known one.
    
    $ = \frac{1}{1-(-x)} = \sum_{n=0}^{\infty} (-x)^n = \sum_{n=0}^{\infty} (-1)^n x^n$
    
    Converges $|x|<1$
    
    \item ex) $\frac{1+x}{1-x}$
    
    $ = (1+x) \sum_{n=0}^{\infty} x^n$ 
    
    $= \sum_{n=0}^{\infty} (1+x) x^n $
    
    $= \sum_{n=0}^{\infty} x^n+x^{n+1}$
    
    ...
    
    $= 1+\sum_{n=1}^{\infty}2x^n$
    
    \item ex) $\frac{3}{1+x^2}$
    
    $=3\frac{1}{1-(-x^2)}$ 
    
    $=3\sum_{n=0}^{\infty} (-x^2)^n$
    
    $=\sum3(-1)^n x^2n$
\end{enumerate}

\subsection{Theorem}

If a function $f$ is defined by the power series $\sum c_n(x-a)^n$ with radius of convergence $R>0$, then $f$ is differentiable on the interval $(a-R, a+R)$ and 
\begin{enumerate}
    \item $f'(x) = \sum n c_n (x-a)^{n-1}$
    \item $\int f(x) \,dx = C + \sum \frac{c_n (x-a)^{n+1}}{n+1}$
\end{enumerate}

and the radii of convergence of $f'(x)$ and $\int f(x) \,dx$ are both $R$.

\subsubsection{Theorem Examples}

\begin{enumerate}
    \item ex) $tan^{-1}(x) = $
    
    \textit{Note:} $\frac{d}{dx} tan^{-1}(x) = \frac{1}{1+x^2} = \sum (-1)^n x^{2n}$
    
    Therefore: $tan^{-1}(x) = \int \sum (-1)^n x^{2n} \,dx$
    
    $=\sum \int (-1)^n x^{2n} \,dx$
    
    $=\sum \frac{(-1)^n x^{2n+1}}{2n+1}$
    
    $= x - \frac{x^3}{3} + \frac{x^5}{5} - \frac{x^7}{7} + ...$
\end{enumerate}

\subsubsection{Notes}

Above theorem can be useful for computing derivatives, integrals.

\begin{enumerate}
    \item $\int_0^1 \frac{1}{1+x^4} \,dx$
    
    This integral is difficult to solve with substitution, etc.
    
    $= \int_0^1 \sum_{n=1}^{\infty} (-1)^n x^{4n} \,dx$
    
    Now, just a polynomial.
    
    $= \sum \frac{(-1)^n x^{4n+1}}{4n+1 } |_0^1$
    
    Radius of convergence is 1, so we are good.
    
    $ = \sum_{n=0}^{\infty} \frac{(-1)^n}{4n+1}$
    
    Using this, we can approximate the integral to any degree of accuracy we want with partial sums.
\end{enumerate}

\section{Taylor Series}

\subsection{Approximating Functions} 

We can approximate a function at a certain range by using its derivative (tangent line) at that point.

\textbf{Tangent line approx}:$p_1(x) = f(a) + f'(a)(x-a) \approx f(x)$ when x is "near" a.

In this case, $p_1(a) = f(a)$ and $p_1'(a) = f'(a)$

By extension, a quadratic approximate of $f$ would have:

$p_2(a) = f(a)$

$p_2'(a) = f'(a)$

$p_2''(a) = f''(a)$

We can try: 

$p_2(x) = f(a) + f'(a)(x-a) + c(x-a)^2$

This is the same as the tangent line approximation but with a quadratic part added ($c(x-a)^2$).

$p_2'(x) = f'(a) + 2c(x-a)$

$p_2''(x) = 2c$

Since we want $p_2''(a) = f''(a)$, we can let $c = \frac{f''(a)}{2}$, and rewrite:

$p_2(x) = f(a) + f'(a)(x-a) + \frac{f''(x)}{2} (x-a)^2$

Following this pattern, we can continue with this pattern for more powers...

$p_3 = f(a) + f'(a)(x-a) + \frac{f''(a)}{2} (x-a)^2 + \frac{f^{(3)}(x)}{3!} (x-a)^3$

This patterns becomes...

\subsection{Taylor Polynomials}

The Taylor polynomial $p_k(x)$ of $f$ is defined by

$p_k(x) = \sum_{n=0}^{k} \frac{f^{(n)}(a)}{n!} (x-a)^n$

This approximates $f$ near $a$. Derivatives $p_k'(a)$ through $p_k^{(k)}(a)$ will match those of $f$.

We can take this idea to infinity and get...

\subsection{Taylor Series}

If a function $f$ can be represented as a power series at $a$, then its power (Taylor) series is 

$f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!} (x-a)^n$.

With infinite terms, this Taylor "approximation" should be equal to our function, if it can have a power series representation. 

\subsubsection{Examples}

\begin{enumerate}
    \item ex) Determine the Taylor series of $e^x$ at $x=0$.
    
    If it has a power series expansion, this is rather easy as all derivatives of any order of $e^x$ are $e^x$, and $f^{(n)}(0)$ always equals $1$.
    
    Taylor series = $1+x+\frac{x^2}{2!} +\frac{x^3}{3!} + \frac{x^4}{4!} + ...$

\end{enumerate}

\subsection{When do we have a Taylor series?}

For any $k$, $f(x) = p_k(x) + R_k(x)$, where $R_k(x)$  is a remainder. This remainder is smaller if $k$ is bigger and/or $x$ is close to $a$.

\textbf{Theorem:} If $|f^{(n+1)}(x)| \leq M$ for $|x-a| \leq d$, then $|R_k(x)| \leq \frac{M}{(k+1)!}|x-a|^{k+1}$ for values of $|x-a| \leq d$.

By extension, if $\lim_{k\to\infty} R_k(x) = 0$, then the Taylor series works.

\subsection{Examples}

\begin{enumerate}
    \item ex) Determine the Taylor series of $cos(x)$ at $x=\pi$
    
    $f(\pi) = -1$
    
    $f'(\pi) = -sin(x)|_{x=\pi} - 0$
    
    $f''(\pi) = -cos(x)|_{x=\pi} = 1$
    
    $f^{(3)}(\pi) = 0$
    
    $f^{(4)}(\pi) = -1$
    
    ...

    $= -1 + 0*(x-\pi) + \frac{1}{2}(x-\pi)^2 + \frac{0}{3!} (x-\pi)^3 + \frac{-1}{4} (x-\pi)^4 + \frac{0}{5!} (x-\pi)^5 + ....$
    
    $= \sum_{n=0}^{\infty} \frac{(-1)^{n+1} (x-\pi)^{2n}}{(2n)! }$
    
    Does this work?
    
    Yes, the limit of $R(x)$ goes to 0.
    
    \item ex) Determine the Maclaurin series of $(1+x)^k$
    
    \textbf{Maclaurin:} Taylor series at $x=0$
    
    If $k = 1, 2, 3...$, it is quite easy as you can just expand.
    
    $f'(x) = k(1+x)^{k-1}$
    
    $f''(x) = k(k-1)(1+x)^{k-2}$
    
    ...
    
    $f^{(n)}(x) = k(k-1)...(k-n+1)(1+x)^{k-n}$
    
    About 0:
    
    $(1+x)^k = \sum_{n=0}^{\infty} \frac{k(k-1)...(k-n+1)}{n!} x^n$ (Binomial expansion)
    
\end{enumerate}

\subsection{Taylor Approximations of Functions at a Point}

ex) Suppose you approximate $x^{\frac{2}{3}}$ about $a=1$ using a cubing Taylor polynomial. Find a bound on the maximum error when $0.8 \leq x \leq 1.2$.

$f(1) = 1$

$f'(1) = \frac{2}{3}x^{-\frac{1}{3}}|_{x=1} = \frac{2}{3} $ 

$f''(1) = -\frac{2}{9}x^{-\frac{4}{3}}|_{x=1} = -\frac{2}{9}$

$f^{(3)}(1) = \frac{8}{27}x^{-\frac{7}{3}}|_{x=1} = \frac{8}{27}$

$f(x) \approx 1+ \frac{2}{3}(x-1) -\frac{\frac{2}{9}}{2}(x-1)^2 + \frac{\frac{8}{27}}{6}(x-1)^3$

$f(\frac{2}{3}) \approx ... \approx 1.129283945$

To bound $R_3(x)$, we need $f^{(4)}(x) = \frac{56}{81}x^{-\frac{10}{3}}$

$|f^{(4)}(x)| \leq |f^{(4)}(0.8)| = 1.454576$

So, $|R_3(x)| \leq \frac{1.454576}{4!} |1.2-1|^4 = \mathbf{0.00009697}$

\section{Differential Equations}
\textbf{What is it?} An equation that involves a function and its derivative, and come up often in real-world situations.

Ex. A proposed model of population growth:

$\frac{dP}{dt} = kP$

The rate of graph of a population $P$ is proportional to the size of the population.

Ex 2. More nuanced model of population growth:

$\frac{dP}{dt} = kP(1-\frac{P}{M})$

\subsection{Terminology}

\begin{enumerate}
    \item A differential equation is an equation that involves an unknown function and one or more of its derivatives.
    
    Ex) $y'' - y = 2e^x$
    \item A solution to a differential equation is a function that makes that equation true.
    
    Ex) $y = xe^x$
    
    \item The order of a differential equation is the highest derivative that appears.
\end{enumerate}

\subsection{Initial Value Problems}

Ex. Verify that $f(x) = ce^{\frac{x^2}{2}}$ are solutions to the differential equation $y'=xy$, and find the solution satisfying the initial condition $y(0) = 2$.

$y= ce^{\frac{x^2}{2}}$

$y' = xce^{\frac{x^2}{2}}$

$y' = xy$


$2 = ce^0$

$c=2$

\subsection{Visualizing Solutions}

Some diff equations are hard to solve....Instead of solving, we can visualize the solutions.

\subsubsection{Direction Fields}

Ex. $y' = y^2-4$

Using this, we know the slope at different points. $(0,0): -4, (0,1): -3, (0,2): 0...$

We can use this to create a direction field and visualize the path the graph will take overall.

\subsection{Euler's Method}

A method to numerically approximate solutions.

Ex. Approximate the solution to $y'=2-y$ with $y(0) = 0$.

$e(0,0), y'=2$ 

$e(1/2,1), y'=1$

$e(1,3/2), y'=1/2$

$e(3/2, 7/4), y'=1/4$

\subsubsection{Definition of the Method}

Given the initial value problem $y'=F(x,y)$, $y(x_0) = y_0$, Euler's method with step size $h$ approximates the solution via: 

$x_n = x_{n-1} + h$

and 

$y_n = y_{n-1} + hF(x_{n-1}, y_{n-1 })$

The smaller $h$, the more accurate.

Ex. Approximate the solution to $y'=y-2x$ with $y(1)=0$

Let's pick $h=0.1$:

$e(1,0): y'=-2$

$e(1.1, 0-2(0.1) = e(1.1, -0.2): y'=-2.4$

$e(1.2, -0.2-2.4(0.1)) = e(1.2, -0.44): y' = ....$

...

\subsection{Separable Equations}

Def: A differential equation that can be written in the form: 

$\frac{dy}{dx} = g(x)f(y)$


Ex: $y'=3x sin(y)$

\subsubsection{Solving}

Rewrite: 

$dy = g(x)f(y) dx$

$\frac{1}{}f(y) dy = g(x) dx$

You can now integrate both sides and solve.

Ex. Solve $\frac{dy}{dx} = xy$

$\int 1/y dy = \int x dx$

$ln|y| = \frac{x^2}{2} + C$

$y=e^{\frac{x^2}{2}+C}$

...

$y=ce^{\frac{x^2}{2}}$


Ex. Solve $\frac{dy}{dt} = \frac{ln(t)}{y+1}$

$\int y+1 dy = \int ln(t) dt$

$\frac{y^2}{2} + y = t ln t - t + C $

$y^2 + 2y = 2t ln t - 2t + C$

(add one to both sides...)

$(y+1)^2 = 2t ln t - 2t + 1 + C$

$y = +/- \sqrt{2t ln t - 2t + 1 + C} -1$

\section{Population Growth Models}
\subsection{Natural Growth}

$\frac{dP}{dt} = kP$

In this model, the rate of growth is proportional to the population size.
 
Solve: 

$\int 1/p dP = \int k dt$

$ln |P| = kt + C$

$P = e^{kt+C}$

$P = P_0e^{kt}$ where $P(0) = P_0$, the "initial" population

\subsection{Logistic Model}

$\frac{dP}{dt} = kP$ where $P$ is small, and $\frac{dP}{dt}$ is small when P approaches its carrying capacity M.

$\frac{dP}{dt} = kP(1-\frac{P}{M})$

Solve: 

$\int \frac{1}{P(1-\frac{P}{M})} = \int k dt$

...

$ln |P| - ln|M-P| = kt+C$

$ln|\frac{M-P}{P}| = -kt+C$

$\frac{M-P}{P} = e^{-kt+C}$

$\frac{M}{P} -1 = Ae^{-kt}$

$P = \frac{M}{1+Ae^{-kt}}$

\subsection{Other Models}

$\frac{dP}{dt} = kP(1-\frac{P}{M}) - c$ (constant rate of loss)

$\frac{dP}{dt} = kP(1-\frac{P}{M})(1-\frac{m}{P})$ (if $P<m$, pop. decreases)

\section{Linear Equations}

A \textit{first-order linear} differential equation is one that can be written in the form:

$\frac{dy}{dx} + P(x)y = Q(x)$

Linear in $y$ and $y'$, not necessarily in $x$.

ex) $y' + sin(x)y = e^x$

\subsection{Example}

$y' + \frac{1}{x} y = x^2$

$xy' + y = x^3$

$(xy)' = x^3$

$xy = \frac{x^4}{4} + C$

$y = \frac{x^3}{4} + C/x$

\subsection{General Method of Solving}

$\frac{dy}{dx} + P(x)y = Q(x)$

We want to multiply by something, $I(x)$, so that $(I(x)\frac{dy}{dx}+I(x)P(x)y) = (I(x)y)'$

Then, we can solve:

$(I(x)y)' = I(x)Q(x)$

$I(x)y = \int I(x) Q(x) dx$

$y = \frac{1}{I(x)} \int I(x) Q(x) dx$

What is $I(x)$, or the "integrating factor", then?

IF $I(x)y' + I(x)P(x)y = (I(x)y)'$ 

then $I(x)y' + I(x)P(x)y = I'(x)y + I(x)y'$

$I(x)P(x) = \frac{dI}{dx}$

$P(x)dx = \frac{1}{I} dI$

$\int P(x) dx = ln |I|$

$\mathbf{e^{\int P(x) dx} = I }$

\subsubsection{Example}
ex) $y' + y = sin(e^x)$

$P(x) = 1$

$I(x) = e^{\int P(x) dx} = e^x$

$e^x y' + e^x y = e^x sin(e^x)$

$(e^x y)' = e^x sin(e^x)$

$\int (e^x y)' dx = \int e^x sin(e^x) dx$

$e^x y = -cos(e^x)+C$

$y = \frac{-cos(e^x}{e^x} + \frac{C}{e^x}$

\section{Predator-Prey System}

\subsection{Assumptions of the System}

\begin{enumerate}
    \item If no predators, the population of prey will grow unconstrained.
    \item If no prey, the population of predators will decline.
    \item Death rate of prey depends on number of interactions between prey and predator.
    \item Growth rate of predators depends on number of interactions between prey and predator.
    \item Number of interactions of prey and predator is proportional to the product of their populations.
\end{enumerate}

\subsection{The Model}

$\frac{dR}{dt} = kR - aRW$ and $\frac{dW}{dt} = -rW + bRW$

In the first equation: The growth rate of rabbits ($R$) is proportional to the number of rabbits, minus the number of rabbits eaten ($aRW$).

In the second equation: The growth rate of wolves ($W$) is proportional to the number of wolves and rabbits, and will decline if there are no rabbits, plus the number of wolves eating rabbits ($bRw$).

\subsection{Example}

$\frac{dR}{dt} = 0.03R - 0.0004RW$ and $\frac{dW}{dt} = -0.01W + 0.002RW$

\textit{* Note:} if $R=0$ and $W=0$, then $\frac{dR}{dt} = 0$ and $\frac{dW}{dt} = 0$. We can then call $(0,0)$ and \textbf{equilibrium point}.

By the chain rule: $\frac{dW}{dt} = \frac{dW}{dR}*\frac{dR}{dt}$

Therefore, $\frac{dW}{dR} = \frac{-0.01W + 0.002RW}{0.03R - 0.0004RW}$

If graphed on a phase plane, you can see a near-cyclical relationship between the two populations, with an equilibrium point near the center.

\subsection{Finding the Equilibrium Point}

This is the point where both derivatives are 0.

$0 = 0.03R - 0.0004RW$

$0 = R(0.03-0.0004W)$

$\mathbf{W = 75}$

$0 = -0.01W + 0.002RW$

$\mathbf{R = 5}$

\subsection{Visualizing Individual Populations}

Other than just graphing the relationship between the two populations, we can also graph the individual populations of the rabbits and wolves fairly easily.

\section{Second-Order Linear Equations}

A second-order homogeneous linear differential equation with constant coefficients is one that can be written in the form:

$ay''+by'+cy=0$ 

where $a,b,c$ are real constants.

second-order: involves $y'$

homogeneous: $=0$

linear: $y'',y',y$

constant coefficients: $a,b,c$ are real numbers

\subsection{Solutions}

\begin{enumerate}
    \item Fact 1: if $y_1(x)$ and $y_2(x)$ are two solutions to
    
    $ay''+by'+cy=0$,
    
    then so are $c_1 y_1(x) + c_2 y_2(x)$ are any real numbers $c_1, c_2$
    
    \item Fact 2: if $y_1(x)$ and $y_2(x)$ are two linearly independent (not multiples of each other) solutions to
    
    $ay''+by'+cy=0$,
    
    then all solutions are of the form $c_1 y_1(x) + c_2 y_2(x)$ for some real numbers $c_1,c_2$.
\end{enumerate}

\subsection{Guessing a Solution}

ex) $y''+3y'-4y=0$

Maybe $e^{rx}$?

$y=e^{rx}, y'=re^{rx}, y''=r^2 e^{rx}$

$r^2e^{rx}+3re^{rx}-4e^{rx}=0$

$e^{rx}(r^2+3r-4)=0$

$e^{rx}(r+4)(r-1)=0$

$r=1,-4$

By our theorem, our general solution is $c_1 e^x+c_2 e^{-4x}$

\subsection{General Method}

$ay''+by'+cy=0$

Guess $e^{rx}$, etc

$e^{rx}(ar^2+br+c)=0$

So $e^{rx}$ is a solution if $ar^2+br+c=0$ (auxiliary equation).

\subsubsection{Cases}

The auxiliary equation $ar^2+br+c=0$ may have:

\begin{enumerate}
    \item two distinct real roots, $r_1$ and $r_2$. The general solution is $c_1 e^{r_1 x}+c_2 e^{r_2 x}$, happens when discriminant is greater than 0.
    
    \item one single real root, $r$. The general solution is $c_1 e^{rx}+c_2 x e^{rx}$, happens when discriminant is equal to 0.
    
    \item two complex roots, $r_1$ and $r_2$. The general solution is $e^{\alpha x}(c_1 cos(\beta x) + c_2 sin(\beta x)$, happens when discriminant is less than 0.
\end{enumerate}

\textbf{Example of single real root}: $y''-4y'+4y=0$

$r^2-4r+4=0$

$(r-2)^2=0$

$r=2$

We know that $e^{2x}$ is a solution. We can also see (through some math) that $xe^{rx}$ is also a solution.

\textbf{Example of two complex roots:} $y''+4y'+5y=0$

$r^2+4r+5=0$

$r=\frac{-4\pm \sqrt{16-20}}{2}$

$r=-2\pm i$

We could say that the solution is $c_1 e^{(-2+i)x}+c_2 e^{(-2-i)x}$

Let's simplify this with Euler's Formula, $e^{i\theta}=cos(\theta)+i sin(\theta)$

General solution: $c_1 e^{(\alpha+i\beta)x}+c_2e^{(\alpha-i\beta)x}$

$=c_1 e^{\alpha x}(cos(\beta x)+i sin(\beta x))+c_2 e^{\alpha x}(cos(-\beta x)+i sin(-i \beta x))$

$=c_1 e^{\alpha x}cos(\beta x)+i c_1 e^{\alpha x} sin(\beta x) + c_2 e^{\alpha x} cos(\beta x)-i c_2 e^{\alpha x} sin(\beta x)$

$=e^{\alpha x}(c_1+c_2) cos(\beta x)+e^{\alpha x} i (c_1-c_2) sin(\beta x)$

say $C_1 = c_1+c_2$ and $C_2 = c_1-c_2$

$=C_1(e^{\alpha x} cos(\beta x)+C_2 e^{\alpha x} sin(\beta x)$

$=e^{\alpha x}(C_1 cos(\beta x))+C_2 sin(\beta x)$


\subsection{Boundary Value Problems}

ex) Find the solution to $y''-6y'+9y=0$ such that $y(0)=2$ and $y(3)=1$

$(r-3)^2=0, r=3$

General solution: $c_1 e^{3x}+c_2 x e^{3x}$

$y(0)=2,c_1=2$

$y(3)=1, 2e^9+3c_2 e^9=1, c_2 = \frac{e^{-9}-2}{3}$

\subsection{NON-Homogeneous Second-Order Linear Differential Equations with Constant Coefficients}

$ay''+by''+cy=G(x)$

Complementary equation: $ay''+by'+cy=0$, solution: $y_c(x)$

Say we can find a particular solution, $y_p(x)$

Theorem: The general solution to the non-homogeneous equation is given by $y_c(x)+y_p(x)$

\subsubsection{Solving; Two methods}

\begin{enumerate}
    \item Undetermined Coefficients
    \begin{enumerate}
        \item easier, straightforward
        \item like guess and check ... kind of?
        \item doesn't always work
    \end{enumerate}
    \item harder
    \item works more widely
\end{enumerate}

\subsubsection{Undetermined Coefficients}

\begin{enumerate}
    \item Take a reasonable guess at $y_p(x)$, but leave the coefficients undetermined (good guess? Same form as $G(x)$)
    \item Solve to find what the coefficients should be
\end{enumerate}

ex) $y''-y=x^2+2$

Guess: $y_p(x)=Ax^2+Bx+C$

$y'_p(x)=2Ax+B$

$y''_p(x)=2A$

$2A-(Ax^2+Bx+C)=x^2+2$

$-Ax^2-Bx+2A-C=x^2+2$

$A=-1, B=0, C=-4$

Therefore, we claim that $-x^2-4$ is a particular solution to this equation.

Complementary equation: $r^2-1=0$, $r=\pm 1$, $c_1 e^x+c_2e^{-x}$

Therefore, our general solution is $c_1 e^x+c_2 e^{-x}+(-x^2+4)$

ex) $y''+y=sin(x)$

Watch out: homogeneous equation has solution of the form $c_1 cos(x)+c_2 sin(x)$

Normally, we would try $y_p(x)=A sin(x) + B cos(x)$, but since is the same form as the homogeneous solution, it won't work. Instead, try multiplying by x; $y_p=Ax sin(x)+Bx cos(x)$

\subsubsection{Variation of Parameters}

Solution to complementary: $c_1 y_1(x)+c_2 y_2(x)$

We can view $c_1$ and $c_2$ as our parameters, and we can vary them to look for a particular solution of the form $y_p(x)=u_1 (x) y_1(x) + u_2(x) y_2(x)$

Then $y'_p(x)=u_1'y_1+u_1 y'_1+u'_2 y_2+u_2 y'_2$

We need to satisfy 2 conditions: 1) $u_1, u_2$ make $u_1 y_1+u_2 y_2$ a solution and 2) say $u'_1 y_1+u'_2 y_2=0$, to make life easier

$y''_p = u'_1y_1'+u_1 y_1''+u'_2 y''_2+u_2 y_2''$

(Assume $y_p$ is a solution)

$a(u'_1 y'_1+u_1 y''_1+u'_2 y'_2+u_2 y_2)+b(u_1 y'_1+u_2 y'_2)+c(u_1 y_1 + u_2 y_2) = G$

$u_1(ay''_1+by'_1+cy_1) + u_2(ay''_2+by'_2+cy_2)+a(u'_1y'_1+u'_2 y'_2)= G$

However, since $ay''_1+by'_1+cy_1$ and $ay''_2+by'_2+cy_2$ are solutions to the homogeneous equation, they just equal 0.

$a(u'_1y'_1+u'_2y'_2)=G$

We also assume that $u'_1 y_1+u'_2 y_2=0$, and if we can satisfy both equations, we can find a particular solution.

\textbf{Example}: $y''-2y'-3y=e^x$

Homogeneous: $(r-3)(r+1)=0$, $r=3,-1$

$y_c = c_1 e^{3x}+c_2 e^{-x}$

Suppose $y_p = u_1 e^{3x}+u_2 e^{-x}$

$y'_p = u'_1 e^{3x}+3u_1 e^{3x}+u'_2 e^{-x}-u_2e^{-x}$

Let $u'_1 e^{3x}+u'_2 e^{-x}=0$

$y'' = 3u'_1 e^{3x}+9u_1 e^{3x}-u'_2 e^{-x}+u_2 e^{-x}$

$3u'_1 e^{3x}+9u_1 e^{3x}-u'_2 e^{-x}+u_2 e^{-x}-2(3u_1 e^{3x}-u_2 e^{-x})-3(3u_1 e^{3x}-u_2 e^{-x})=e^{x}$

$u_1(9e^{3x}-6e^{3x}-3e^{3x})+u_2 (e^{-x}+2e^{-x}-3e^{-x})+3u'_1 e^{3x}-u'_2 e^{-x}= e^{x}$

$3u'_1 e^{3x} - u'_2 e^{-x}=e^x$

solve with both equations...

$y_p(x) = \frac{e^{-3x}}{8}e^{3x}+ \frac{-e^{2x}}{8}e^{-x}= \frac{-e^{x}}{4}$

\textbf{General solution:} $c_1 e^{3x}+ c_2 e^{-x} - \frac{e^x}{4}$

\section{Second-Order Linear Equations Applications: Springs}

\subsection{Hooke's Law}

If a spring is stretched or compressed $x$ units from its nature length, the corresponding force exerted is proportional to the distance $x$, Force = $-kx$, where $k$ is called the \textit{spring constant}.

\subsection{Newton's Second Law}

$f=ma$

Since acceleration is the second derivative of distance, we can combine this with Hooke's Law: $m \frac{d^2 x}{d t^2} = -kx$, or $mx''+kx=0$, a homogeneous differential equation.

\subsection{Simple Harmonic Motion}

$mx''+kx=0$

$mr^2+k=0$

$r^2=-k/m$

$r=\pm i \sqrt{\frac{k}{m}}$

\textbf{Solution:} $c_1 cos(\sqrt{\frac{k}{m}}t)+c_2 sin(\sqrt{\frac{k}{m}}t)$

\textit{Often written as $x(t)=A cos(\omega t+ \delta)$, where $\omega=\sqrt{\frac{k}{m}}$, $A=\sqrt{c_1^2+c_2^2}$, and $cos(\delta) = \frac{c_1}{A}$, $sin(\delta)=\frac{-c_2}{A}$. This form makes it clearer that the function has a frequency and amplitude, and is periodic.}

\subsection{Example}

ex) A spring with a 5-kg mass is kept stretched 0.5 m beyond its natural length by a force of 40 N. The spring starts at its equilibrium point and is given an initial velocity of 0.2 m/s. Find the position of the mass at any time $t$.

$40 = k(0.5)$

$80 = k$

$5x'' = -80x$

$5x'' + 80x = 0$

$5r^2+80 = 0$

...

$r = \pm 2i$

$\mathbf{x(t) = c_1 cos(2t) + c_2 sin(2t)}$

$x(0) = 0$

$0 = c_1 cos(0) + c_2 sin(0)$

$\mathbf{0 = c_1}$

$x'(t)=-2c_1 sin(2t) + 2c_2 cos(2t)$

$x'(0) = 0.2 => 2c_2 = 0.2$

$\mathbf{c_2 = 0.1}$

\textbf{Solution:} $x(t) = 0.2 sin(2t)$

\subsection{Damping Force}

Damping force is proportional to the velocity of the mass:

Damping Force = $-c \frac{dx}{dt}$
 
Adding this to our earlier equation: 

$F = ma = -kx - c \frac{dx}{dt}$

$mx'' + cx'+kx=0$

$mr^2+cr+k=0$

$r = \frac{-c \pm \sqrt{c^2-4km}}{2m}$

We have to look at the three possible cases for the discriminant here:

\begin{enumerate}
    \item over-damping: $c^2-4mk>0$
    
    roots will be less than 0, giving us exponential decay ($x=c_1 e^{r_1 t}+c_2 e^{r_2 t}$)
    \item critical damping: $c^2-4mk = 0$
    
    gets back to equilibrium as quickly as possible ($x = c_1e^{rt} + c_2 x e^{rt}$)
    \item under-damping $c^2-4mk<0$
    
    oscillates ($x=e^{-rt}(c_1 cos(\omega t)+c_2 sin(\omega t)$)
\end{enumerate}

\section{Using Power Series}

\subsection{Why?}

It is a lot easier to work with and find solutions that can't be expressed as function when looking at a function as a power series when solving differential equations.

\subsection{Examples}

\begin{enumerate}
    \item $y''-y=0$

Assume that $y=\sum_{n=0}^{\infty} c_n x^n = c_0 + c_1 x + c_2 x^2...$

$y' = \sum_{n=1}^{\infty} n c_n x^{n-1}c_1 2c_2 x+3c_3 x^2...$

$\sum_{n=1}^{\infty}n c_n^{n-1}-\sum_{n=0}^{\infty} c_n x^n = 0$

$\sum_{n=0}^{\infty}(n+1)c_{n+1}x^{n}-\sum_{n=0}^{\infty} c_n x^n = 0$

$\sum ((n+1)c_{n+1}- c_n)x^n=0$

Therefore, $(n+1)c_{n+1}-c_n = 0$ for all $n$.

Value for $c_0$ implies values for the rest; a recursive relationship.

Say: $c_0 = 1$

$c_1 = 1$

$c_2 = \frac{1}{2}$

$c_3 = \frac{1}{3*2}$

$c_4 = \frac{1}{4*3*2}$

$\mathbf{c_n = \frac{1}{n!}}$

$\mathbf{y = \sum_{n=0}^{\infty} \frac{x^n}{n!} = e^x}$

\item $y''-xy'-y=0$

For this problem, our normal methods don't work.

Say: $y=\sum_{n=0}^{\infty} c_n x^n$

$y' = \sum_{n=1}^{\infty} n c_n x^{n-1}$

$y'' = \sum_{n=2}^{\infty} n(n-1)c_n x^{n-2}$

$\sum_{n=2}^{\infty} n(n-1)c_n x^{n-2} - x\sum_{n=1}^{\infty} n c_n x^{n-1} - \sum_{n=0}^{\infty} c_n x^n = 0$

\textit{(simplify and rewrite starting at $n=0$)}

$\sum_{n=0}^{\infty} (n+2)(n+1)c_{n+2} x^{n} - \sum_{n=0}^{\infty} n c_n x^n - \sum_{n=0}^{\infty} c_n x^n = 0$

$\sum_{n=0}^{\infty} ((n+1)(n+1) c_{n+2} - n c_n - c_n)x^n = 0$

$(n+2)(n+1)c_{n+2}-(n+1)c_n = 0$

$c_{n+2} = \frac{c_n}{n+2}$ (\textit{This is our recursive relationship})

$c_0 = c_0$

$c_1 = c_1$

$c_2 = \frac{c_0}{2}$

$c_3 = \frac{c_1}{3}$

$c_4 = \frac{c_0}{8}$

$c_5 = \frac{c_1}{15}$

You can separate this out by even vs odd terms.

$y = \sum_{n=0}^{\infty} \frac{c_0 x^{2n})}{(2n)(2n-2)(2n-4)...(2)}+\sum_{n=0}^{\infty} \frac{c_1 x^{2n+1}}{(2n+1)(2n-1)...(1)}$

$=\sum_{n=0}^{\infty} \frac{c_0 x^{2n}}{2^n n!} - \sum_{n=0}^{\infty} \frac{c_1 x^{2n+1}}{\frac{(2n+1)!}{2^n n!}}$

$=\sum_{n=0}^{\infty} \frac{c_0 (\frac{x^2}{2})^n}{n!} + ...$

$=c_0 e^{\frac{x^2}{2}} + \sum_{n=0}^{\infty} \frac{c_1 x^{2n+1}}{(2n+1)!}$

Although we were able to find an elementary function for our left side, there is none for our right side, and therefore must be left in power series notation. In practice, we could approximate this value to any degree of accuracy with previously discussed methods.
\end{enumerate}







\end{document}

