// ! external
#import "../typst/notes.typ": *
#import "../typst/shortcuts.typ": *

// ! configuration
#show: doc => conf(
  course_code: "MATH356",
  course_title: "Probability",
  subtitle: "",
  semester: "Fall 2024",
  professor: "Prof. Asoud Asgharian",
  doc
)
#set align(left)

#pagebreak()
= Prerequisites
// TODO starts here
#definition("limsup, liminf of sets")[
   Let ${A_n}_(n >= 1)$ be a sequence of sets. We define $
   overline(lim)_(n-> infinity) = limsup_(n-> infinity) A_n := {x : x in A_n "for infinitely many" n} = sect.big_(n=1)^infinity union.big_(k=n)^infinity A_k
   $ and $
   underline(lim)_(n-> infinity) = liminf_(n->infinity) A_n := {x : x in A_n "for all but finitely many" n} = union.big_(n=1)^infinity sect.big_(k=n)^infinity A_k.
   $ If $liminf A_n = limsup A_n$, we say $A_n$ _converges_ to this value and write $lim_(n-> infinity) A_n = liminf A_n = limsup A_n$
]

#proposition[
  $liminf A_n subset.eq limsup A_n$
]

#example[
  Let $A_n = {n}$. Then $liminf A_n = limsup A_n = nothing = lim A_n$. Let $A_n = {(-1)^n}$. Then $liminf A_n = nothing, limsup A_n = {-1, 1}$.
]

#definition("sigma-field")[A non-empty class of subsets of a set $Omega$ which is closed under countable unions and complement, and contains $nothing$ is called a _$sigma$-field_ or _$sigma$-algebra_.]

#definition("Borel sigma-algebra")[The $sigma$-algebra generated by the class of all bounded, semi-closed intervals is called the _Borel algebra_ of subsets of $RR$, denoted $frak(B), frak(B)(RR)$.]

#theorem[Every countable set is Borel.]

#proof[
  ${x} = sect.big_(n=1)^infinity (x - 1/n, x]$ for any $x in RR$, so $A := {x_n : n in NN} = union.big_(n=1)^infinity {x_n} in frak(B)$.
]

#theorem[$frak(B) = sigma({"open sets in "RR})$.]

= Probability
== Sample Space
#definition("Random/statistical experiment")[
   A _random/statistical experiment_ (stat. exp.) is one in which 
   + all outcomes are known in advance;
   + any performance of the experiment results in an outcome that is not known in advance;
   + the experiment can be repeated under identical conditions.
]

#definition("Sample space")[The _sample space_ of a stat. exp. is the pair $(Omega, cal(F))$ where $Omega$ the set of all possible outcomes and $cal(F)$ a $sigma$-algebra of subsets of $Omega$.

We call points $omega in Omega$ _sample points_, $A in cal(F)$ _events_. If $Omega$ countable, we call $(Omega, cal(F))$ a _discrete sample space_.
]

#definition[Let $(Omega, cal(F))$ be a sample space. A set function $P$ is called a _probability measure_ or simply _probability_ if 
1. $P(A) >= 0$ for all $A in cal(F)$
2. $P(Omega) = 1$
3. For ${A_n} subset.eq cal(F)$, disjoint, then $P(union.big_(n=1)^infinity A_n) = sum_(n=1)^infinity P(A_n)$.
]

#theorem[$P$ monotone ($A subset.eq B => P(A) <= P(B)$) and subtractive $P(B \\ A) = P(B) - P(A)$.]

#theorem[For all $A, B in cal(F), P(A union B) = P(A) + P(B) - P(A sect B)$.]

#corollary[$P$ subadditive; for any $A, B in cal(F), P(A union B) <= P(A) + P(B)$.]

#corollary[$P(A^c) = 1 - P(A)$.]

#theorem("Principle of Inclusion/Exclusion")[Let $A_1, dots, A_n in cal(F)$. Then $
P(union.big_(k=1)^n A_k) &= sum_(k=1)^n P(A_k) \
&- sum_(k_1 < k_2) P(A_(k_1) sect A_(k_2))\
&+ sum_(k_1 < k_2 < k_3) P(A_(k_1) sect A_(k_2) sect A_(k_3)) \
&+ dots + (-1)^(n) P(sect.big_(k=1)^n A_k).
$]

#theorem("Bonferroni's Inequality")[For $A_1, dots, A_n$, $
sum_(i=1)^n P(A_i) - sum_(i < j) P(A_i sect A_j) <= P(union.big_(i=1)^n A_i) <= sum_(i=1)^n P(A_i).
$]

#theorem("Boole's Inequality")[$P(A sect B) >= 1 - P(A^c) - P(B^c)$.]

#corollary[For ${A_n} subset.eq cal(F)$, $
P(sect_(n=1)^infinity A_n) >= 1 - sum_(n=1)^infinity P(A_n^c)
$]

#theorem("Implication Rule")[If $A, B, C in cal(F)$ and $A$ and $B$ imply $C$ (i.e. $A sect B subset.eq C$) then $P(C^c) <= P(A^c) + P(B^c)$.]

#theorem("Continuity")[Let ${A_n} subset.eq cal(F)$ non-decreasing i.e. $A_n supset.eq A_(n-1) forall n$, then $
lim_(n->infinity) P(A_n) = P(union.big_(n=1)^infinity A_n).
$ Let ${A_n}$ non-increasing, then $
lim_(n-> infinity) P(A_n) = P(sect.big_(n=1)^infinity A_n).
$ Finally, more generally, for ${A_n}$ such that $lim_(n-> infinity) A_n = A$ exists, then $
P(A) = lim_(n-> infinity) P(A_n).
$
]

= Combinatorics - Finite $sigma$-fields
== Counting

We consider now $Omega = {omega_1, dots, omega_n}$ finite sample spaces, and consider $cal(F) = 2^Omega$.

#definition("Permutation")[An ordered arrangement of $r$ distinct objects is called a permutation. The number of ways to order $n$ distinct objects taken $r$ at a time is $
sans(P)_r^n = (n!)/(n-r)!.
$]

#definition("Combination")[The number of combinations of $n$ objects taken $r$ at a time is the number of subsets of size $r$ that can be formed from $n$ objects, $
sans(C)_r^n = mat(n; r) = sans(P)_r^n / r! = n! / (r! (n-r)!).
$]
// TODO extend permutation?
#theorem[The number of unordered arrangements of $r$ objects out of a total of $n$ objects when sampling with replacement is $
mat(n + r - 1; r).
$]

== Conditional Probability

#theorem[
Let $A, H in cal(F)$. We denote by $P(A | H)$ the probability of $A$ given $H$ has occured. We have, in particular, $
P(A | H) = P(A sect H)/(P(H)),
$ if $P(H) eq.not 0$.
]

#definition[We say two events $A, B$ are independent if $P(A | B) = P(A)$, or equivalently $P(A sect B) = P(A) P(B)$.]

#proposition("Multiplication Rule")[
$
P(sect.big_(j=1)^n A_j) = product_(i=1)^n P(A_i | sect_(j=0)^(i-1) A_j),
$ taking $A_0 := Omega$ by convention.
]

#proposition("Law of Total Probability")[
Let ${H_n} subset.eq cal(F)$ be a partition of $cal(F)$, namely $H_i sect H_j = nothing$ for all $i eq.not j$, and $union_(j=1)^infinity H_j = Omega$. If $P(H_n) >0 forall n$, then $
P(B) = sum_(n=1)^infinity P(B | H_n) P(H_n) forall B in cal(F).
$
]

#theorem("Baye's")[
  Let ${H_n}$ be a partition of $Omega$ with all strictly nonzero measure and let $B in cal(F)$ with nonzero measure. Then $
  P(H_n | B) = (P(H_n) P(B | H_n))/(sum_(n=1)^infinity P(H_n) P(B | H_n)).
  $
]

#definition("Mutual Independence")[
A family of sets $cal(A)$ is said to be _mutually independent_ iff $forall$ finite sub collections ${A_(i_1), dots, A_(i_k)}$, the following holds $
P(sect_(j=1)^k A_(i_j)) = product_(j=1)^k P(A_(i_j)).
$
]

= Random Variables and Probability Distributions

We tacitly fix some sample space $(Omega, cal(F))$.

#definition("Random Variable")[A real-valued function $X : Omega -> RR$ is called a _random variable_ or _rv_ if $
X^(-1) (B) in cal(F)
$ for all $B in borel$.]

#theorem[$X$ an rv $<=>$ for all $x in RR$, $
{X <= x} in cal(F).
$ ]

#theorem[If $X$ a rv, then so is $a X + b$ for all $a, b in RR$.]

#theorem[
Fix an rv $X$ defined on a probability space $(Omega, cal(F), P)$. Then, $X$ induces a measure on the sample space $(RR, borel)$, denote $Q$ and given by $
Q (B) := P(X^(-1) B)
$ for any Borel set $B$.
]

#remark[If $X$ a random variable, then the sets ${X = x}, {a < x <= b}, {X < x},$ etc are all events.]

#definition("Distribution Function")[An $RR$-valued function $F$ that is non-decreasing, right-continuous and satisfies $
F(-infinity) = 0, F(+infinity) = 1
$ is called a _distribution function_ or _df_.
]

#theorem[${x | F "discontinuous"}$ is at most countable.]

#definition[Given a random variable $X$ and a probability space $(Omega, cal(F), P)$, we define the df of $X$ as $
F(x) = P(X <= x).
$]

#remark[It is not obvious a priori that this is indeed a df.]

#theorem[If $Q$ a probability on $(RR, borel)$, then there exists a df $F$ where $
F(x) = Q(-infinity, x],
$ and conversely, given a df $F$, there exists a unique probability on $(RR, borel)$.]

== Discrete and Continuous Random Variables

#definition[$X$ called "discrete" if $exists$ countable set $Epsilon subset RR$ such that $P(X in Epsilon) = 1$.]

#proposition[Suppose $Epsilon = {x_n}_(n=1)^(infinity)$ and put $p_n := P(X = x_n)$. Then, $
sum_(n=1)^infinity p_n = 1,
$ where ${p_n}$ defines a non-negative sequence.]

#definition("PMF")[
  Such a sequence ${p_n}$ satisying $0 <= p_n = P(X = x_n)$ for a sequence ${x_n}$  and $sum p_n = 1$ is called a _probability mass function_ (pmf) of $X$. Then, $
  F_X (x) = P_X ((-infinity, x]) = sum_(n : x_n <= x) p_n
  $ and $
  X(omega) = sum_(n=1)^infinity x_n bb(1)_({X = x_n}) (omega).
  $
]

#definition[
  $X$ called _continuous_ if $F$ induced by $X$ is absolutely continuous, i.e. if there exists a non-negative function $f(t)$ such that $
  F(x) = integral_(-infinity)^x f(t) dif t
  $ for all $x in RR$. Such a function $f$ is called the _probability density function_ (pdf) of $X$.
]

#theorem[Let $X$ continuous with pdf $f$. Then $
P(B) = integral_B f(t) dif t
$ for every $B in frak(B)_RR$.]

#theorem[Every nonnegative real function $f$ that is integrable over $RR$ and such that $integral_(-infinity)^infinity f(x) dif x = 1$ is the PDF of some continuous $X$.]

== Functions of a Random Variable

#theorem[Let $X$ be an rv and $g$ a Borel-measurable function on $RR$. Then, $g(X)$ also an rv.]

#theorem[Let $Y = g(X)$ as above. Then, $P(Y <= y) = P(X in g^(-1) (-infinity, y])$.]

#example[
  Let $X$ be an RV with Poisson distribution; we write $X tilde "Poisson"(lambda)$; where $
  P(X = k) = (e^(-lambda) lambda^k)/k!
  $ for $k in NN union {0}$. Let $Y = X^2 + 3$. We say that $X$ has _support_ {0, 1, 2, dots} (more generally, where $X$ can take values), and so $Y$ has support on ${3, 4, 7, dots} =: B$. Then $
  P(Y = y) = P(X = sqrt(y - 3)) = (e^(-lambda) lambda^(sqrt(y - 3)))/(sqrt(y-3)!),
  $ for $y in B$ and $P(Y = y) = 0$ for $y in.not B$.
]

#theorem[Let $X$ cont. rv with pdf f_X. Let $Y = g(X)$ be differentiable for all $x$ and with either strictly positive or negative derivative. Then, $Y = g(X)$ also a continuous rv with pdf given by $
h(y) = cases(f_x (g^(-1)(y)) |dif/(dif y) g^(-1)(y)| "for" alpha < y < beta, 0 "else") ,
$ where $
alpha := min {g(-infinity), g(infinity)}, beta := max{g(-infinity), g(infinity)}.
$]

#theorem[Let $X$ continuous rv with cdf $F_X (x)$. Let $Y = F_X (X)$. Then, $Y tilde "Unif (0, 1)"$.]

#proof[
  $
  P(Y <= y) &= P(F_X (X) <= y)\
  &= P(X  <= F_X^(-1) (y)).
  $ // TODO
]

#theorem[
  Let $X$ continuous rv with pdf $f_X$ and $y = g(x)$ // TODO just extension of the previous previous one.
]

= Moments and Moment Generating Functions
#definition("Expected Value")[Let $X$ be a discrete (continuous) rv with PMF (PDF) $p_k = P(X = x_k)$ ($f$). If $sum |x_k| p_k < infinity$ ($integral |x| f_X (x) dif x < infinity$) then we say the _expected value_ of $X$ exists, and write $
EE(X) = sum x_k p_k (= integral x dot f(x) dif x).
$]

#theorem[
If $X$ symmetric about $alpha in RR$, i.e. $P(X >= alpha + x) = P(X <= alpha - x)$ for all $x in RR$ (or in the continuous case, $f(alpha - x) = f(alpha + x)$), then $EE(X) = alpha$.
]

#theorem[
  Let $g$ Borel-measurable and $Y = g(X)$. Then, $
  EE(Y) = sum_(j=1)^infinity g(x_j) P_X (X = x_j).
  $ If $X$ continuous, $
  = integral g(x) f(x) dif x.
  $
]
#definition[
For $alpha > 0$, we say $EE(|X|^alpha)$ (if it exists) is the _$alpha$-th moment_ of $X$.
]

#example[
  Let $X$ such that $P(X = k) = 1/N$, $k = 1, dots, N$, namely $X tilde "Unif"_({1, dots, N})$. Then $
  EE(X) = sum_(k=1)^N k/N = (N+1)/2.
  $
]

#theorem[If the $t$th moment of $X$ exists, so does the $s$th moment for $s < t$.]

#theorem[If $EE(|X|^k) < infinity$ for some $k > 0$, then $
n^k P(|X| > n) -> 0
$ as $n-> infinity$.]


== Variance
Let $X$ a random variable. Put $mu_X := EE[X]$. We define the _variance_ of $X$, denoted $sigma_X^2$, by $
sigma_X^2 = "Var"(X) = EE[(X - mu_X)^2]
$ or eqiuvalently $
"Var"(X) &= EE[X^2] - 2 mu_X EE[X] + EE[mu_X^2]\
&= EE[X^2] - 2 mu_X^2 + mu_X^2 = EE[X^2]-EE[X]^2
$ Let $S tilde "Bin"(n,p)$. Then, $"Var"[S] = EE[S^2]- (n p)^2$. To compute $EE[S^2] = EE[S(S-1) + S]$, we may abuse combinatorial identities and eventually find $
"Var"[S] = n p (1 - p).
$

== Some Particular Distributions
=== Hypergeometric

Consider a population of $N$ objects, and a subpopulation of $M$ objects. Let $X_i$ be a random variable equal to 1 if a sampled object is from the $M$-subpopulation, $0$ else, and put $Y = sum_(i=1)^n X_i$. Then, $
P(Y = k) = (mat(M;k) mat(N-M;n-k))/(mat(N;n))
$ for any $k = 0, dots, n$. We have $
EE[Y] &= 1/(mat(N;n)) sum_(k=0)^n k  mat(M;k) mat(N-M; n-k)\
&=1/(mat(N;n)) sum_(k=0)^n cancel(k)  (M!)/(cancel(k)(k-1)! (N-k)!) mat(N-M;n-k)\
&= M/(mat(N;n)) sum_(k=0)^n mat(M-1; k-1) mat(N-M;n-k)\
&= M/(mat(N;n))sum_(k=0)^(n-1) mat(M-1; k) mat(N-M; (n-1)-k)\
&= M/(mat(N;n)) mat(N-1; n-1) \
&= M dot (n! (N-n)! (N-1)!)/(N! (n-1)! (N-n)!)\
&= M (n/N).
$

=== Uniform Distribution

Let $X$ be a discrete uniformly distributed random variable, with $P(X = x) = 1/N$ for $x in {1, dots, N}$ (one typically writes $X tilde "unif"{1,N}$). Then, $
EE[X] = sum_(k=1)^N k/N = N(N+1)/(2N) = (N+1)/2.
$

=== Binomial Distribution


Let $X_i$ for $i = 1,dots, n$ be a discrete boolean rv with $P(X_i = 1) = p, P(X_i = 0) = 1-p$. Put $S = sum_(i=1)^n X_i$. We say $S$ has binomial distribution, and write $
S tilde "Bin"(n,p).
$ Then, we have that $
P(S) = mat(n; k) p^k (1-p)^(n-k) 
$ and so $
EE[S] = sum_(k=0)^n k P(S = k) = dots.c = n p.
$
An easier way to compute this is by using the linearity of $EE$, namely, $
EE[S] = EE[sum_(i=1)^n X_i] = sum_(i=1)^n EE[X_i] = sum_(i=1)^n 1 dot p + 0 dot (p - 1) = n p.
$