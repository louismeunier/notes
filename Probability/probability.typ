// ! external
#import "../typst/notes.typ": *
#import "../typst/shortcuts.typ": *

// ! configuration
#show: doc => conf(
  course_code: "MATH356",
  course_title: "Probability",
  subtitle: "",
  semester: "Fall 2024",
  professor: "Prof. Asoud Asgharian",
  doc
)
#set align(left)

#pagebreak()
= Prerequisites
// TODO starts here
#definition("limsup, liminf of sets")[
   Let ${A_n}_(n >= 1)$ be a sequence of sets. We define $
   overline(lim)_(n-> infinity) = limsup_(n-> infinity) A_n := {x : x in A_n "for infinitely many" n} = sect.big_(n=1)^infinity union.big_(k=n)^infinity A_k
   $ and $
   underline(lim)_(n-> infinity) = liminf_(n->infinity) A_n := {x : x in A_n "for all but finitely many" n} = union.big_(n=1)^infinity sect.big_(k=n)^infinity A_k.
   $ If $liminf A_n = limsup A_n$, we say $A_n$ _converges_ to this value and write $lim_(n-> infinity) A_n = liminf A_n = limsup A_n$
]

#proposition[
  $liminf A_n subset.eq limsup A_n$
]

#example[
  Let $A_n = {n}$. Then $liminf A_n = limsup A_n = nothing = lim A_n$. Let $A_n = {(-1)^n}$. Then $liminf A_n = nothing, limsup A_n = {-1, 1}$.
]

#definition("sigma-field")[A non-empty class of subsets of a set $Omega$ which is closed under countable unions and complement, and contains $nothing$ is called a _$sigma$-field_ or _$sigma$-algebra_.]

#definition("Borel sigma-algebra")[The $sigma$-algebra generated by the class of all bounded, semi-closed intervals is called the _Borel algebra_ of subsets of $RR$, denoted $frak(B), frak(B)(RR)$.]

#theorem[Every countable set is Borel.]

#proof[
  ${x} = sect.big_(n=1)^infinity (x - 1/n, x]$ for any $x in RR$, so $A := {x_n : n in NN} = union.big_(n=1)^infinity {x_n} in frak(B)$.
]

#theorem[$frak(B) = sigma({"open sets in "RR})$.]

= Probability
== Sample Space
#definition("Random/statistical experiment")[
   A _random/statistical experiment_ (stat. exp.) is one in which 
   + all outcomes are known in advance;
   + any performance of the experiment results in an outcome that is not known in advance;
   + the experiment can be repeated under identical conditions.
]

#definition("Sample space")[The _sample space_ of a stat. exp. is the pair $(Omega, cal(F))$ where $Omega$ the set of all possible outcomes and $cal(F)$ a $sigma$-algebra of subsets of $Omega$.

We call points $omega in Omega$ _sample points_, $A in cal(F)$ _events_. If $Omega$ countable, we call $(Omega, cal(F))$ a _discrete sample space_.
]

#definition[Let $(Omega, cal(F))$ be a sample space. A set function $P$ is called a _probability measure_ or simply _probability_ if 
1. $P(A) >= 0$ for all $A in cal(F)$
2. $P(Omega) = 1$
3. For ${A_n} subset.eq cal(F)$, disjoint, then $P(union.big_(n=1)^infinity A_n) = sum_(n=1)^infinity P(A_n)$.
]

#theorem[$P$ monotone ($A subset.eq B => P(A) <= P(B)$) and subtractive $P(B \\ A) = P(B) - P(A)$.]

#theorem[For all $A, B in cal(F), P(A union B) = P(A) + P(B) - P(A sect B)$.]

#corollary[$P$ subadditive; for any $A, B in cal(F), P(A union B) <= P(A) + P(B)$.]

#corollary[$P(A^c) = 1 - P(A)$.]

#theorem("Principle of Inclusion/Exclusion")[Let $A_1, dots, A_n in cal(F)$. Then $
P(union.big_(k=1)^n A_k) &= sum_(k=1)^n P(A_k) \
&- sum_(k_1 < k_2) P(A_(k_1) sect A_(k_2))\
&+ sum_(k_1 < k_2 < k_3) P(A_(k_1) sect A_(k_2) sect A_(k_3)) \
&+ dots + (-1)^(n) P(sect.big_(k=1)^n A_k).
$]

#theorem("Bonferroni's Inequality")[For $A_1, dots, A_n$, $
sum_(i=1)^n P(A_i) - sum_(i < j) P(A_i sect A_j) <= P(union.big_(i=1)^n A_i) <= sum_(i=1)^n P(A_i).
$]

#theorem("Boole's Inequality")[$P(A sect B) >= 1 - P(A^c) - P(B^c)$.]

#corollary[For ${A_n} subset.eq cal(F)$, $
P(sect_(n=1)^infinity A_n) >= 1 - sum_(n=1)^infinity P(A_n^c)
$]

#theorem("Implication Rule")[If $A, B, C in cal(F)$ and $A$ and $B$ imply $C$ (i.e. $A sect B subset.eq C$) then $P(C^c) <= P(A^c) + P(B^c)$.]

#theorem("Continuity")[Let ${A_n} subset.eq cal(F)$ non-decreasing i.e. $A_n supset.eq A_(n-1) forall n$, then $
lim_(n->infinity) P(A_n) = P(union.big_(n=1)^infinity A_n).
$ Let ${A_n}$ non-increasing, then $
lim_(n-> infinity) P(A_n) = P(sect.big_(n=1)^infinity A_n).
$ Finally, more generally, for ${A_n}$ such that $lim_(n-> infinity) A_n = A$ exists, then $
P(A) = lim_(n-> infinity) P(A_n).
$
]

= Combinatorics - Finite $sigma$-fields
== Counting

We consider now $Omega = {omega_1, dots, omega_n}$ finite sample spaces, and consider $cal(F) = 2^Omega$.

#definition("Permutation")[An ordered arrangement of $r$ distinct objects is called a permutation. The number of ways to order $n$ distinct objects taken $r$ at a time is $
sans(P)_r^n = (n!)/(n-r)!.
$]

#definition("Combination")[The number of combinations of $n$ objects taken $r$ at a time is the number of subsets of size $r$ that can be formed from $n$ objects, $
sans(C)_r^n = mat(n; r) = sans(P)_r^n / r! = n! / (r! (n-r)!).
$]
// TODO extend permutation?
#theorem[The number of unordered arrangements of $r$ objects out of a total of $n$ objects when sampling with replacement is $
mat(n + r - 1; r).
$]

== Conditional Probability

#theorem[
Let $A, H in cal(F)$. We denote by $P(A | H)$ the probability of $A$ given $H$ has occured. We have, in particular, $
P(A | H) = P(A sect H)/(P(H)),
$ if $P(H) eq.not 0$.
]

#definition[We say two events $A, B$ are independent if $P(A | B) = P(A)$, or equivalently $P(A sect B) = P(A) P(B)$.]

#proposition("Multiplication Rule")[
$
P(sect.big_(j=1)^n A_j) = product_(i=1)^n P(A_i | sect_(j=0)^(i-1) A_j),
$ taking $A_0 := Omega$ by convention.
]

#proposition("Law of Total Probability")[
Let ${H_n} subset.eq cal(F)$ be a partition of $cal(F)$, namely $H_i sect H_j = nothing$ for all $i eq.not j$, and $union_(j=1)^infinity H_j = Omega$. If $P(H_n) >0 forall n$, then $
P(B) = sum_(n=1)^infinity P(B | H_n) P(H_n) forall B in cal(F).
$
]

#theorem("Baye's")[
  Let ${H_n}$ be a partition of $Omega$ with all strictly nonzero measure and let $B in cal(F)$ with nonzero measure. Then $
  P(H_n | B) = (P(H_n) P(B | H_n))/(sum_(n=1)^infinity P(H_n) P(B | H_n)).
  $
]

#definition("Mutual Independence")[
A family of sets $cal(A)$ is said to be _mutually independent_ iff $forall$ finite sub collections ${A_(i_1), dots, A_(i_k)}$, the following holds $
P(sect_(j=1)^k A_(i_j)) = product_(j=1)^k P(A_(i_j)).
$
]

= Random Variables and Probability Distributions

We tacitly fix some sample space $(Omega, cal(F))$.

#definition("Random Variable")[A real-valued function $X : Omega -> RR$ is called a _random variable_ or _rv_ if $
X^(-1) (B) in cal(F)
$ for all $B in borel$.]

#theorem[$X$ an rv $<=>$ for all $x in RR$, $
{X <= x} in cal(F).
$ ]

#theorem[If $X$ a rv, then so is $a X + b$ for all $a, b in RR$.]

#theorem[
Fix an rv $X$ defined on a probability space $(Omega, cal(F), P)$. Then, $X$ induces a measure on the sample space $(RR, borel)$, denote $Q$ and given by $
Q (B) := P(X^(-1) B)
$ for any Borel set $B$.
]

#remark[If $X$ a random variable, then the sets ${X = x}, {a < x <= b}, {X < x},$ etc are all events.]

#definition("Distribution Function")[An $RR$-valued function $F$ that is non-decreasing, right-continuous and satisfies $
F(-infinity) = 0, F(+infinity) = 1
$ is called a _distribution function_ or _df_.
]

#theorem[${x | F "discontinuous"}$ is at most countable.]

#definition[Given a random variable $X$ and a probability space $(Omega, cal(F), P)$, we define the df of $X$ as $
F(x) = P(X <= x).
$]

#remark[It is not obvious a priori that this is indeed a df.]

#theorem[If $Q$ a probability on $(RR, borel)$, then there exists a df $F$ where $
F(x) = Q(-infinity, x],
$ and conversely, given a df $F$, there exists a unique probability on $(RR, borel)$.]

== Discrete and Continuous Random Variables

#definition[$X$ called "discrete" if $exists$ countable set $Epsilon subset RR$ such that $P(X in Epsilon) = 1$.]

#proposition[Suppose $Epsilon = {x_n}_(n=1)^(infinity)$ and put $p_n := P(X = x_n)$. Then, $
sum_(n=1)^infinity p_n = 1,
$ where ${p_n}$ defines a non-negative sequence.]

#definition("PMF")[
  Such a sequence ${p_n}$ satisying $0 <= p_n = P(X = x_n)$ for a sequence ${x_n}$  and $sum p_n = 1$ is called a _probability mass function_ (pmf) of $X$. Then, $
  F_X (x) = P_X ((-infinity, x]) = sum_(n : x_n <= x) p_n
  $ and $
  X(omega) = sum_(n=1)^infinity x_n bb(1)_({X = x_n}) (omega).
  $
]

#definition[
  $X$ called _continuous_ if $F$ induced by $X$ is absolutely continuous, i.e. if there exists a non-negative function $f(t)$ such that $
  F(x) = integral_(-infinity)^x f(t) dif t
  $ for all $x in RR$. Such a function $f$ is called the _probability density function_ (pdf) of $X$.
]

#theorem[Let $X$ continuous with pdf $f$. Then $
P(B) = integral_B f(t) dif t
$ for every $B in frak(B)_RR$.]

#theorem[Every nonnegative real function $f$ that is integrable over $RR$ and such that $integral_(-infinity)^infinity f(x) dif x = 1$ is the PDF of some continuous $X$.]

== Functions of a Random Variable

#theorem[Let $X$ be an rv and $g$ a Borel-measurable function on $RR$. Then, $g(X)$ also an rv.]

#theorem[Let $Y = g(X)$ as above. Then, $P(Y <= y) = P(X in g^(-1) (-infinity, y])$.]

#example[
  Let $X$ be an RV with Poisson distribution; we write $X tilde "Poisson"(lambda)$; where $
  P(X = k) = (e^(-lambda) lambda^k)/k!
  $ for $k in NN union {0}$. Let $Y = X^2 + 3$. We say that $X$ has _support_ {0, 1, 2, dots} (more generally, where $X$ can take values), and so $Y$ has support on ${3, 4, 7, dots} =: B$. Then $
  P(Y = y) = P(X = sqrt(y - 3)) = (e^(-lambda) lambda^(sqrt(y - 3)))/(sqrt(y-3)!),
  $ for $y in B$ and $P(Y = y) = 0$ for $y in.not B$.
]

#theorem[Let $X$ cont. rv with pdf f_X. Let $Y = g(X)$ be differentiable for all $x$ and with either strictly positive or negative derivative. Then, $Y = g(X)$ also a continuous rv with pdf given by $
h(y) = cases(f_x (g^(-1)(y)) |dif/(dif y) g^(-1)(y)| "for" alpha < y < beta, 0 "else") ,
$ where $
alpha := min {g(-infinity), g(infinity)}, beta := max{g(-infinity), g(infinity)}.
$]

#theorem[Let $X$ continuous rv with cdf $F_X (x)$. Let $Y = F_X (X)$. Then, $Y tilde "Unif (0, 1)"$.]

#proof[
  $
  P(Y <= y) &= P(F_X (X) <= y)\
  &= P(X  <= F_X^(-1) (y)).
  $ // TODO
]

#theorem[
  Let $X$ continuous rv with pdf $f_X$ and $y = g(x)$ // TODO just extension of the previous previous one.
]

= Moments and Moment Generating Functions
#definition("Expected Value")[Let $X$ be a discrete (continuous) rv with PMF (PDF) $p_k = P(X = x_k)$ ($f$). If $sum |x_k| p_k < infinity$ ($integral |x| f_X (x) dif x < infinity$) then we say the _expected value_ of $X$ exists, and write $
EE(X) = sum x_k p_k (= integral x dot f(x) dif x).
$]

#theorem[
If $X$ symmetric about $alpha in RR$, i.e. $P(X >= alpha + x) = P(X <= alpha - x)$ for all $x in RR$ (or in the continuous case, $f(alpha - x) = f(alpha + x)$), then $EE(X) = alpha$.
]

#theorem[
  Let $g$ Borel-measurable and $Y = g(X)$. Then, $
  EE(Y) = sum_(j=1)^infinity g(x_j) P_X (X = x_j).
  $ If $X$ continuous, $
  = integral g(x) f(x) dif x.
  $
]
#definition[
For $alpha > 0$, we say $EE(|X|^alpha)$ (if it exists) is the _$alpha$-th moment_ of $X$.
]

#example[
  Let $X$ such that $P(X = k) = 1/N$, $k = 1, dots, N$, namely $X tilde "Unif"_({1, dots, N})$. Then $
  EE(X) = sum_(k=1)^N k/N = (N+1)/2.
  $
]

#theorem[If the $t$th moment of $X$ exists, so does the $s$th moment for $s < t$.]

#theorem[If $EE(|X|^k) < infinity$ for some $k > 0$, then $
n^k P(|X| > n) -> 0
$ as $n-> infinity$.]


== Variance
Let $X$ a random variable. Put $mu_X := EE[X]$. We define the _variance_ of $X$, denoted $sigma_X^2$, by $
sigma_X^2 = "Var"(X) = EE[(X - mu_X)^2]
$ or eqiuvalently $
"Var"(X) &= EE[X^2] - 2 mu_X EE[X] + EE[mu_X^2]\
&= EE[X^2] - 2 mu_X^2 + mu_X^2 = EE[X^2]-EE[X]^2
$ Let $S tilde "Bin"(n,p)$. Then, $"Var"[S] = EE[S^2]- (n p)^2$. To compute $EE[S^2] = EE[S(S-1) + S]$, we may abuse combinatorial identities and eventually find $
"Var"[S] = n p (1 - p).
$

#theorem[
  If $X$ a random variable and $g$ a Borel-measurable function, then $
  EE[g(X)] &= integral_(-infinity)^infinity h(x) f_X (x) dif x "if continuous"\
  EE[g(X)] &= sum_(k=0)^infinity h(x_k) p_k "if discrete"
  $
]

== Some Particular Distributions
=== Hypergeometric

Consider a population of $N$ objects, and a subpopulation of $M$ objects. Let $X_i$ be a random variable equal to 1 if a sampled object is from the $M$-subpopulation, $0$ else, and put $Y = sum_(i=1)^n X_i$. Then, $
P(Y = k) = (mat(M;k) mat(N-M;n-k))/(mat(N;n))
$ for any $k = 0, dots, n$. We have $
EE[Y] &= 1/(mat(N;n)) sum_(k=0)^n k  mat(M;k) mat(N-M; n-k)\
&=1/(mat(N;n)) sum_(k=0)^n cancel(k)  (M!)/(cancel(k)(k-1)! (N-k)!) mat(N-M;n-k)\
&= M/(mat(N;n)) sum_(k=0)^n mat(M-1; k-1) mat(N-M;n-k)\
&= M/(mat(N;n))sum_(k=0)^(n-1) mat(M-1; k) mat(N-M; (n-1)-k)\
&= M/(mat(N;n)) mat(N-1; n-1) \
&= M dot (n! (N-n)! (N-1)!)/(N! (n-1)! (N-n)!)\
&= M (n/N).
$

=== Uniform Distribution

Let $X$ be a discrete uniformly distributed random variable, with $P(X = x) = 1/N$ for $x in {1, dots, N}$ (one typically writes $X tilde "unif"{1,N}$). Then, $
EE[X] = sum_(k=1)^N k/N = N(N+1)/(2N) = (N+1)/2.
$

=== Binomial Distribution


Let $X_i$ for $i = 1,dots, n$ be a discrete boolean rv with $P(X_i = 1) = p, P(X_i = 0) = 1-p$. Put $S = sum_(i=1)^n X_i$. We say $S$ has binomial distribution, and write $
S tilde "Bin"(n,p).
$ Then, we have that $
P(S) = mat(n; k) p^k (1-p)^(n-k) 
$ and so $
EE[S] = sum_(k=0)^n k P(S = k) = dots.c = n p.
$
An easier way to compute this is by using the linearity of $EE$, namely, $
EE[S] = EE[sum_(i=1)^n X_i] = sum_(i=1)^n EE[X_i] = sum_(i=1)^n 1 dot p + 0 dot (p - 1) = n p.
$
== MGF, PGF

#definition("PGF")[
Given a discrete random variable $X$ with support in the naturals and pmf ${p_k}$, the _probability generating function_ (PGF) of $X$ is defined as $
P(s) = sum_(k=0)^infinity p_k s^k,
$ wherever this series converges.
]

#proposition[
$1/(n!) P^((n)) (0) = p_n$ for any $n >= 0$. Similarly, $
P'(s)|_(s=1) = EE[X], wide P''(s)|_(s = 1) = EE[X (X-1)], wide dots
$
]

#example[
  If $X tilde "Poisson"(lambda)$, then $
  P(s) &= sum_(k=0)^infinity (e^(-lambda) lambda^k)/(k!) s^k \ 
  &= e^(-lambda) sum_(k=0)^infinity ((lambda s)^k)/(k!)  \
  &= e^(-lambda) e^(lambda s) = e^(lambda (s - 1)).
  $
]

#definition("MGF")[
The _moment generating function_ (MGF) of a continuous random variable $X$ with pdf $f_X$ is given by, where the integral converges, $
M_X (s) := EE[e^(s X)] = integral_(-infinity)^infinity e^(s x) f_X (x) dif x.
$
]

#example[
  Let $X tilde exp(lambda)$ for $lambda > 0$, namely $
  f_X (x) = lambda e^(- lambda x)
  $ for $x > 0$, $0$ otherwise. Then, $
  M_X (s) = integral_(0)^infinity e^(s x) lambda e^(lambda x) dif x = dots.c = (lambda/(lambda - s)) "if" lambda > s,
  $ and does not exist (does not converge) otherwise.
]

#theorem[
  A given MGF uniquely determines a CDF, and conversely, if an MGF of a random variable exists, it is unique.
]

#theorem[
  If $M_X (s)$ exists for $|s| < s_0$, then the derivatives of $M_X$ of all order exists at $s = 0$, and can be evaluated under the integral; namely, $
  M_X^((k))(s)|_(s = 0) = EE[X^k], wide k > 0.
  $ Ie, $
  M_X (s) = sum_(k=0)^infinity M^((k)) (0) (s^k)/(k!)
  $
]

#example[
  Let $X tilde "Geom"(p)$, $1 > p > 0$, where $p_k = p(1-p)^(k-1), k = 1, dots$. Then, $
  // P(X = n)
  $
]

== Moment Inequalities

#theorem("Markov's")[
  Let $h(x) >= 0$ a function of a random variable $X$. If $EE[h(X)]$ exists, then for every $epsilon > 0$, $
  P({h(X) >= epsilon}) <= (EE[h(X)])/(epsilon).
  $
]
#proof[
  Let $A = {h >= epsilon}$ and denote the pdf of $X$ by $f_X$. Then, $
  EE[h(X)] &= integral_(-infinity)^infinity h(x) f_X (x) dif x \
  &= integral_(A) h(x) f_X (x) dif x + integral_(A^c) h(x) f_X (x) dif x \ 
  &>= epsilon integral_(A) f_X (x) dif x = epsilon P({X in A}) = epsilon P({h(X) >= epsilon}).
  $
]

#corollary[For any $k, r > 0$, $
P{|X| >= k} <= (EE[ |X|^r])/k^r.
$]

#proof[
  Let $epsilon = k^r$, $h(x) = |x|^r$.
]

#corollary("Chebychev's Inequality")[
  Let $mu_X = EE[X]$ and $sigma_X^2 = "Var"(X)$. Then, for any $k >= 1$, $
  P({|X - mu_X| > k sigma}) <= 1/(k^2).
  $
]
#proof[
  Let $h(x) = (X - mu_X)^2, epsilon = h^2 sigma_X^2$.
]

#example[
  When $k = 3$, this tells us that the likelihood of $X$ being more than three standard deviations ($sqrt("Var"(X))$) away from the mean is less that $1/(3^2) = 1/9$, i.e. at least $89 percent$ of the time, $X$ is within $3$ stdevs of the mean. We can sharpen this bound in general.
]

#theorem("Gauss's Inequality")[
  Let $X tilde f$ be unimodal with mode $v = "argmax"_x (f(x))$. Put $tau^2 = EE[(X - v)^2]$. Then, $
  P{|X - v| > epsilon} <= cases(
    (4 tau^2)/(9 epsilon^2) "if" epsilon >= tau 2/sqrt(3), 
    1 - epsilon/(tau sqrt(3)) "else".
  )
  $
]

#theorem("Vysochanskij-Petunin (VP) Inequality")[
  Let $X$ unimodal and put $xi^2 = EE[(X - alpha)^2]$ for $alpha in RR$. Then, for any $epsilon > 0$, $
  P(|X - alpha| > epsilon) <=  cases( (4 xi^2)/(9 epsilon^2) "if" epsilon >= xi sqrt(8/3), (4 xi^2)/(9 epsilon^2) - 1/3 "else").
  $
]

#example[
  If $alpha = mu_X =  EE[X]$ and we take $epsilon = sigma_X$, the previous inequality gives that $
  P{|X - mu_X|  > 3 sigma} <= 4/81 approx 0.05,
  $ namely, gives rise to the "3-$sigma$" rule.
]

#theorem[
  Let $X$ be a random variable with $EE[X] = 0, "Var"(X) = sigma^2$. Then, $
  P(X > x) <= sigma^2/(sigma^2 + x^2),wide "for" x > 0 \
  P(X > x) >= x^2/(sigma^2 + x^2),wide "for" x < 0.
  $
]

#lemma[
$|a+b|^r <= C_r (|a|^r + |b|^r)$ where $C_r = cases(1 "if" 0 <= r <= 1, 2^(r - 1) "if" r > 1)$.
]

#theorem[
Let $X, Y$ be random variables and let $r > 0$. If $EE|X|^r, EE|Y|^r$ exist, then $EE|X+Y|^r$ exists, and we have $
EE|X+Y|^r<=C_r (EE|X|^r + EE|Y|^r).
$
]

= Multiple Random Variables

#definition[
  A vector of random variables $bold(X) = (X_1, dots, X_n)$. We say $X_1, dots, X_n$ jointly distributed according to some $F$ if $
  P(bold(X) <= bold(a)) = P(X_1 <= a_1, dots, X_n <= a_n) = F(a_1, dots, a_n).
  $ One must be careful in defining $F$ appropriately, as there are some caveats compared to the one dimensional case. See the text for details.

  We only deal with cases to follow where all random variables are of the same type.
]

#definition("Marginal Distribution")[
  Given $X, Y tilde F$, the marginal cumulative distribution of $X$ is defined as $
  F_1 (x) = lim_(y -> infinity) F(x, y)
  $ if $X, Y$ continuous and $
  F_1 (x_i) = sum_(i : x_i <= 1) sum_(j=1) p_(i j)
  $ when $X$ discrete.

  The marginal pdf of $X$ is given by $
  f_1 (x) = integral_(-infinity)^infinity f (x, y) dif y,
  $ when continuous, and when discrete, $
  P(X = x_i) = p_(i bullet) = sum_(j=1)^infinity p_(i, j)
  $
]

== Conditional Distributions

#definition[
Let $X, Y$ be two jointly distributed random variables. We define $
p_(i|j) = P(X = x_i | Y = y_j) = (p_(i, j))/(p_(bullet, j)),
$ which yields conditional cdf $
F_(X|Y) (x, y) = P(X <= x | Y = y) = (P(X <= x, Y = y))/(P(Y = y)),
$ as long as $P(Y = y) eq.not 0$.

When continuous, we define the conditional cdf $
F_(X|Y) (x, y) = (integral_(-infinity)^x f(y, y) dif u)/(f_2 (y)),
$ where $f_2$ the marginal pdf of $Y$. By extension, the conditional pdf is given by $
f_(X|Y)(x, y) = (f(x, y))/(f_2 (y)).
$
]

#definition[
  We say two jointly distributed random variables $X, Y$ are independent if $
  P(X = x_i, Y = y_j) = P(X = x_i) P(Y = y_j)
  $ for every $x_i, y_j$ in the respective supports if $X, Y$ discrete, and $
  f_(X, Y) (x, y) = f_X (x) f_Y (y)
  $ for every $x, y$ where defined if $X, Y$ continuous. Similarly, $X_1, dots, X_n$ are said to be mutually independent if $
  P_X (X_i in A_i) = product_(i=1)^n P(X_i in A_i)
  $ for every Borel set $A_i$.

  If $X_1, dots, X_n$ are independent, and all have the same marginal distributions, call it $f$, we say $X_1, dots, X_n$ are independent and identically distributed, and write $X_1, dots, X_n tilde^("iid") f$.
]

#definition[
The expected value of a function $h$ of two jointly distributed random variables $X, Y$ is defined to be $
EE[h(X, Y)] = integral_(-infinity)^infinity integral_(-infinity)^infinity h(x, y) f_(X, Y) (x, y) dif x dif y.
$ We defined the conditional expectation $
EE[h(X, Y) | Y = y] = integral_(-infinity)^infinity h(x, y) f_(X|Y)(x|y) dif x.
$
]

#proposition[
  $EE[X] = EE[EE[X|Y]]$, $"Var"(X) = EE["Var"(X|Y)] + "Var"(EE(X|Y))$.
]

#example[
Let $X_i tilde^("iid") f, i = 1, dots, n$. Let $X_((j)) = max_(1 <= i <= j) X_i$. Then, $
P(X_((j)) <= x) = F_(X) (x)^j,
$ where $F_X$ the common cdf of the $X_i$'s.

Let $Y_((j)) = min_(1 <= i <= j) X_i$ Then, $
P(Y_((j)) <= y) = [1 - F_X (x)]^n
$
]

#theorem[
  Let $(X_1, dots, X_n) tilde f_(bold(X)) (bold(x))$. Let $u_i = g_i (bold(x)), i = 1, dots, n$ be continuous one-to-one maps with continuous inverses $x_i = h_i (bold(u)) = h_i (u_1, dots, u_n)$, such that the partials $(partial x_i)/(partial u_j)$ exists and are continuous for $1 <= i, j <= n$, and such that the Jacobian $
  J = (partial (x_1, dots, x_n))/(partial (u_1, dots, u_n)) eq.not 0,
  $ in the range of definition. Then, the new random variable $bold(U) = (U_1, dots, U_n)$ has joint, absolutely continuous distribution function with joint pdf given by $
  w(u_1, dots, u_n) = |J| f_(bold(X)) (h_1 (bold(u)), dots, h_n (bold(u))).
  $
]

#example[Rocket // TODO
]

== Covariance, Correlation

#definition("Covariance, Correlation")[
  Given two random variables $X, Y$ with means $mu_X, mu_Y$ respectively, define $
  "Cov"(X, Y) = EE[(X - mu_X)(Y - mu_Y)].
  $
  Let $sigma_X^2, sigma_Y^2$ be the variance of $X, Y$ respectively, then define, if $sigma_X, sigma_Y > 0$, the correlation of $X, Y$$
  rho(X, Y) = "Cov"(X,Y)/(sigma_X sigma_Y).
  $
]

#proposition[
Correlation is translation, scaling invariant.
]

#proposition[If $X, Y$ independent, $"Cov"(X, Y) = 0$.]

#theorem("Holders")[
  Let $p, q > 1$ such that $1/p + 1/q = 1$. Then, $
  EE[X Y] <= (EE|X|^p)^(1/p) (EE|X|^q)^(1/q).
  $ In particular, if $p = q = 2$, we have the Cauchy-Schwarz inequality, and if $X -> X - mu_X, Y -> Y - mu_Y$, then this tells us $|"Cov"(X, Y)| <= sigma_X sigma_Y$, and hence $|rho(X, Y)| <= 1$.
]

#corollary("Lyapunov's")[
  For every $1 <= r < s < infinity$, $
  (EE|X|^r)^(1/r) <= (EE|X|^s)^(1/s).
  $
]

#corollary("Jensen's")[
Let $g$ be a convex function. Then, $
g(EE[X]) <= EE[g(X)].
$ 
]

#theorem("Minkowski's")[
  Let $p >= 1$. Then, assuming all expectations exist, $
  (EE |X + Y|^p)^(1/p) <= (EE |X|^p)^(1/p) + (EE |Y|^p)^(1/p).
  $
]

= Convergence of Random Variables

#definition("Almost Sure Convergence")[
  Let ${X_n}$ be a sequence of random variables. We say $X_n$ converges to an rv $X_0$ _almost surely_ (a.s.) and write $X_n ->^("a.s.") X_0$ if $
  P{ lim_(n->infinity) X_n = X_0} = 1.
  $
]

#definition("Convergence in Probability")[
  We say $X_n -> X_0$ _in probability_ if for every $epsilon > 0$, $
  lim_(n->infinity) P(|X_n - X_0| > epsilon) = 0.
  $ Compare this to the notation of convergence in measure.
]

#definition("Convergence in Law/Weak Convergence")[
  Suppose $X_n tilde F_n$. If $F_0$ another cdf with $X_0 tilde F_0$, we say $X_n$ converges to $X_0$ in law/distribution/weakly and write $X_n ->^("L") X_0$ or $F_n ->^"W" F_0$ if $
  F_n (x) -> F_0 (x)
  $ for every $x$ such that $F_0$ continuous at $x$.
]

#example[
  Let $0 <= p <= 1$ and let $X_i = {1 "with" p, 0 "with" 1 -p}$. Let $
  T_n = 1/n sum_(i=1)^n X_i
  $ for all $n >= 1$. We claim that $T_n -> p$ in probability.
]

#example[
  Let $X_i tilde^("iid") "Unif"(0, theta)$. Let $M_n = max{X_1, dots, X_n}$. Prove that $M_n -> theta$ in probability.
]