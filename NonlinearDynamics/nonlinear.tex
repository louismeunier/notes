\documentclass[12pt, oneside]{article}
\usepackage{amsthm}
\usepackage{libertine}
\usepackage[margin=0.15in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{multicol}
\usepackage[shortlabels]{enumitem}
\usepackage{siunitx}
\usepackage{setspace}
\usepackage{cancel}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{tabularx}
\usepackage{titlesec}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage[side]{footmisc}
\usepackage{xcolor-solarized}
\usepackage{svg}
\usepackage{physics}
\usepackage[colorlinks=true, linkcolor=darkgray]{hyperref}
% \usepackage{cleveref}
\usepackage[]{csquotes}
\usepackage{cleveref}


\usepackage[createShortEnv]{proof-at-the-end}

% makes theorems, definitions, etc. "restatable" as shown
% can add more with same format as you wish
\renewcommand*{\proofname}{}


\declaretheorem[
  % thmbox=S,
  name=Definition,
  refname={Definition, definition}, numberwithin=section,
  shaded={rulecolor=solarized-blue, rulewidth=2pt}
]{definition}

\declaretheorem[
  % thmbox=S,
  name=Axiom,
  refname={Axiom, axiom},
  numberwithin=section,
  shaded={rulecolor=solarized-orange, rulewidth=2pt}
]{axiom}

\declaretheorem[
  % thmbox=S,
  name=Lemma,
  refname={Lemma, lemma},
  numberwithin=section,
  shaded={rulecolor=solarized-orange, rulewidth=1pt, bgcolor={rgb}{1,1,1}}
]{lemma}

\declaretheorem[
  % thmbox=S,
  name=Corollary,
  refname={Corollary, corollary},
  numberwithin=section,
  shaded={rulecolor=solarized-orange, rulewidth=1pt, bgcolor={rgb}{1,1,1}}
]{corollary}

\declaretheorem[
  % thmbox=S,
  name=Remark,
  refname={Remark, remark},
  numberwithin=section
]{remark}

\declaretheorem[
  % thmbox=S,
  name=Theorem,
  refname={Theorem, theorem},
  numberwithin=section,
  shaded={rulecolor=solarized-red, rulewidth=1pt}
]{theorem}

\declaretheorem[
  % thmbox=M,
  name=Example,
  refname={Example, example},
  numberwithin=section,
  shaded={rulecolor=solarized-cyan, rulewidth=1pt, bgcolor={rgb}{1,1,1}}
]{example}

\declaretheorem[
  % thmbox=S,
  name=Proposition,
  refname={Proposition, proposition},
  numberwithin=section,
  shaded={rulecolor=solarized-magenta, rulewidth=1pt, bgcolor={rgb}{1,1,1}}
]{proposition}


\newEndThm[no proof here, restate, text proof={Examples}, one big link={\emph{See more}}]{definitionEnd}{definition}
\newEndThm[no proof here, restate, one big link = {\emph{Proof}}]{lemmaEnd}{lemma}
\newEndThm[no proof here, restate, one big link = {\emph{(Solution)}}]{exerciseEnd}{exercise}
\newEndThm[no proof here, restate]{axiomEnd}{axiom}

% \newEndThm[proof here, restate]{theoremEnd}{theorem}

% makes "quoted" text actually look correct
\MakeOuterQuote{"}

% page footer
\newpagestyle{mypage}{%
    \footrule
    \setfoot{\small\textcolor{gray}{ยง\thesubsection}}{\small\textcolor{gray}{\textit{\sectiontitle: \textbf{\subsectiontitle}}}}{\textcolor{gray}{\small p. \thepage}}
}

% title page settings
\newcommand{\pageauthor}{Louis Meunier}
\newcommand{\pagetitle}{Nonlinear Dynamics and Chaos}
\newcommand{\pagesubtitle}{MATH376}

% black square for qed symbol
\renewcommand{\qedsymbol}{$\blacksquare$}

\titleformat{\section}
{\centering\normalfont\Large\bfseries}
{\thesection}{1em}{}

\begin{document}
\setstretch{2.25}
\noindent
\begin{center}
    \begin{tabularx}{\textwidth} { 
        >{\raggedright\arraybackslash}X 
        >{\raggedleft\arraybackslash}X}
    \LARGE \pageauthor \\
    \LARGE \textbf{\pagetitle} & \LARGE \textbf{\pagesubtitle}\\
    \end{tabularx}\\
    \rule[2ex]{0.8\textwidth}{1pt}
\end{center}

\setstretch{1.5}
\tableofcontents

% "enables" footer with section+subsection, etc. just comment it out if you don't want it
\pagestyle{mypage}

% makes sections a very dark gray + centered
\titleformat{\section}
{\color{darkgray}\centering\normalfont\Large\bfseries}
{\color{darkgray}\thesection}{1em}{}

% need to change margins and such here for rest of document
% kind of messy but what can you do
\newpage
% modify these as you wish
\newgeometry{margin=0.25in, top=0.4in, bottom=0.5in, marginparwidth=1.4in, marginparsep=0.3in, outer=0.2in, includemp}
\parskip=0.6em

\section{Introduction, Motivations, Overview}

\begin{definition}[Dynamical Systems]
  Systems which evolve over time. We can categorize them as
  \begin{itemize}
    \item \textbf{continuous,} which define ODEs, eg. $\dot{u}(t) = f(u)$, where $u(t)$ is define over some interval $t$;
    \item \textbf{discrete,} which are defined by a \emph{map}, $S : \mathbb{R}^n \longrightarrow \mathbb{R}^n, u \longmapsto S_u$, where $u_1 = S_{u_0}$, $u_2 = S_{u_1}$, etc., where $n \in \mathbb{Z}$.
  \end{itemize}
\end{definition}

\subsection{Examples of continuous dynamical systems}
\subsubsection{Exponential growth/decay}

Consider $\dot{u} = \lambda u$, where $u \in \mathbb{R}$, $\dot{u} = \dv{u}{t}$ ($u = f(t)$), and $\lambda$ is a constant parameter. This is a linear, separable ODE;

\begin{align*}
  \dv{u}{t} &= \lambda u\\
  \int \frac{\dd{u}}{u} &= \int \lambda \dd{t}\\
  \implies u(t) &= u_0 e^{\lambda t}, \text{ where } u(0) = u_0
\end{align*}

Assuming $\lambda \neq 0$ (otherwise $u(t) = u_0$), we can analyze the behavior of $u$ as $t \longrightarrow \infty$ and $t \longrightarrow -\infty$.

\begin{itemize}
  \item Clearly, if $u_0 = 0$, then $u(t) = 0 \forall t$. This is called a \textbf{steady state}.
  \item Else, (under the assumption $u_0 > 0$), we can consider the cases $\lambda > 0$ and $\lambda < 0$.
  \begin{itemize}
    \item $\lambda >0 \implies \lim_{t \to \infty} u(t) = \infty$
    \item $\lambda < 0 \implies \lim_{t \to -\infty} u(t) = 0$
  \end{itemize}
\end{itemize}

Using this fairly simple analysis, we can draw \textbf{phase diagrams} describing how the system changes based on initial conditions, for instance, given $\lambda > 0$:
\begin{figure}[h!]
  \centering
  \includegraphics*[width=0.5\textwidth]{figures/example1-1-1.png}
  \caption{Exponential growth/decay, $\lambda> 0$}
\end{figure}

Note that the phase diagram is \emph{independent} of the value of $\lambda$; naturally, a larger $\lambda$ will result in a "faster" (so to speak) growth/decay, but the "asymptotic" behavior is identical. We can say that the \textbf{dynamics} of the system are independent of the constant $\lambda$.

In this case, at $u_0 = 0$, all other $u_0$ greater than or less than $0$ diverge away from $u_0$; this would be called a \textit{unstable equilibrium}. If $\lambda < 0$, we would see all $u_0$ converging to $0$, which would be an \textit{asymptotically stable equilibrium}.

\subsubsection{Logistic ODE}

Consider the \textit{logistic} ode $\dot{x} = \lambda x (1-x), x(t) \in \mathbb{R}$. Normally, we would solve this ode (using separation of variables, resulting in a messy fraction decomposition, and lots of algebraic manipulation \footnote{Note that this is also a \emph{Bernoulli ODE}, ie one of the form $y' +p(x)y = q(x)y^n$; you can solve it by dividing by $y^n$ and making the substitution $u = y^{1-n}$ to get a far nicer (though obviously equivalent) linear equation which you can solve using an integrating factor.}) This will give the final explicit solution \[x(t) = \frac{1}{(\frac{1}{x_0}-1)e^{-\lambda t} + 1},\] where $x(0) = x_0$. We can then analyze $x(t)$ similar to the previous example. Due to the complexity (the "embedded" exponential, etc), however, this is quite difficult.

\textit{Alternatively}, we can consider the original ode \[\dot{x} = f(x) = \lambda x (1-x),\] without the exact solutions. Assuming $\lambda > 0$ (similar "methodology" for $\lambda < 0$), we can analyze the behavior:

\begin{itemize}
  \item Steady states will occur when $\dot{x} = 0 \implies  \lambda x (1-x) = 0  \implies x = 0 \text{ or } x = 1$.
  \item Next, we can consider the behavior of $x$ in the intervals $(-\infty, 0)$, $(0, 1)$, and $(1, \infty)$.
  \begin{itemize}
    \item $(-\infty, 0)$: as $x \to - \infty$, $\dot{x} \to -\infty$ ($(+) \times (-) \times (+) \sim (-) $)
    \item $(0, 1)$: as $x \to 1$, $\dot{x}$ increases.
    \item $(1, \infty)$: as $x \to \infty$, $\dot{x} \to -\infty$ ($(+) \times (+) \times (-) \sim (-) $)
  \end{itemize}
\end{itemize}

This means that $x_0 = 0$ and $x_0 = 1$ are unstable and stable, respectively. We can then draw the phase diagram:

\begin{figure}[h!]
  \centering
  \includegraphics*[width=0.5\textwidth]{figures/example1-1-2.png}
\end{figure}

We can compare the logistic ODE to the seemingly unrelated\footnote{Also a Bernoulli} $\dot{x} = x-x^3$. Factoring, we write $\dot{x} = x(1-x)(1+x)$, indicating steady states at $x = -1, 0, 1$. This results in a very similar phase diagram:

\begin{figure}[h!]
  \centering
  \includegraphics*[width=0.5\textwidth]{figures/example1-1-2a.png}
\end{figure}

As is clear from the diagram, the two equations have very similar dynamics; however, at no initial condition does the second ODE tend to positive/negative infinity.

\subsection{Analyzing the Lorenz Equations}

The Lorenz equation is defined by \[\begin{cases}
  \dot{x} = \sigma(y-x)\\
  \dot{y} = rx-y - xz\\
  \dot{z} = xy - bz
\end{cases},\] where solutions $u(t) = \begin{pmatrix}
  x(t)\\
  y(t)\\
  z(t)
\end{pmatrix} \in \mathbb{R}^3$. A trivial steady state exists at $(x,y,z) = (0,0,0), \forall r > 0$, and two more exist\footnote{These are fairly easy to find by considering different possible cases that would cause each of $\dot{x}, \dot{y}, \dot{z} = 0.$} $(x,y,z) = (\pm \sqrt{b(r-1)}, \pm \sqrt{b(r-1)}, r-1), \forall r > 1$. Notice that when $r = 1$, the two non-trivial steady states collapse into the trivial steady state. This is what we call a \textbf{bifurcation}, or in this case specifically, a \textbf{pitchfork bifurcation}. This can make sense if we plot \footnote{This is an odd way to look at the system (as the parameter $r$ is suddenly becoming the independent variable), but it is helpful to analyze how exactly the steady states behave due to the parameters.} $(x,y,z)$ of the steady points as a function of $r$:
\begin{figure}[h!]
  \centering
  \includegraphics*[width=0.4\textwidth]{figures/example1-2-1a.png}
\end{figure}

Further analyzing the dynamics of the system is a little trickier - we can't exactly use the same approaches as before in the $\mathbb{R}^2$ space. The system is clearly not linear because of the $xz$ and $xy$ terms. However, if we assume that $x, y,z$ are small and remain small, then we can approximate the system as linear by dropping these terms \footnote{$xy$ and $xz$ would be "very small" if $x, y,z$ are small, as they are funtionally quadratic terms. Intuitively, $\alpha \times \beta \ll 1$ given $\alpha, \beta < 1$.}. Thus, we can approximate the system as $\dot{x} = \sigma(y-x), \dot{y} = rx-y, \dot{z} = -bz$, or, equivalently, \[\begin{pmatrix}
  \dot{x}\\
  \dot{y}\\
  \dot{z}
\end{pmatrix} = \begin{pmatrix}
- \sigma &\sigma &0\\
r & -1 & 0\\
0 & 0 & -b
\end{pmatrix}\cdot \begin{pmatrix}
  x\\
  y\\
  z
\end{pmatrix}.\] Now, if $x(0) = y(0) = 0$, then $\dot{x}=\dot{y} = 0 \implies x(t) = y(t) = 0 \forall t$, and thus the solution evolves solely on the $z$-axis; ie $\dot{z} = -b z \implies z = z(0)e^{-bt}$, which $\to 0$ as $t \to\infty$, supporting our assumption that $x,y,z$ remain small. 

On the other hand, let's assume\footnote{In this context, $\ll$ means "much less".} $|x(t), y(t),z(t)|\ll 1$; again, this results in $x,y,z$ remaining small, and thus "allows" us to study the dynamics approximately near $(x,y,z) = (0,0,0)$. Solving the matrix formulation of the system, we get \[u(t) = \begin{pmatrix}
  x(t)\\
  y(t)\\
  z(t)\end{pmatrix} = c_1 e^{\lambda_1 t} \underline{\mathbf{v_1}} + c_2 e^{\lambda_2 t} \underline{\mathbf{v_2}} + c_3 e^{\lambda_3 t} \underline{\mathbf{v_3}},
\] where $\lambda_i$ are eigenvalues and $\underline{\mathbf{v_i}}$ are corresponding eigenvectors.

Clearly\footnote{Note the $-b$ in the bottom right of the matrix, surround by 0's; the only component of the "position" vector that will multiply to that $-b$ is $z$, thus, any vector with only a $z$ component will remain unchanged, $\begin{pmatrix}0\\0\\1\end{pmatrix}$ being the "unit" of these.}, $\underline{\mathbf{v_3}} = \begin{pmatrix}
  0\\
  0\\
  1
\end{pmatrix}$ with a corresponding $\lambda_3 = -b$. Thus, $\underline{\mathbf{v_1}}$ and $\underline{\mathbf{v_2}}$ must lie in the $xy$-plane, and similarly, are eigenvectors of the top left, $\begin{pmatrix}
  -\sigma & \sigma\\
  r & -1\end{pmatrix}$ (with 0 $z$-component, of course). Solving for these by standard methods, we write
  
\begin{align*}
  0 &= \det\left(\begin{pmatrix} 
    - \sigma & \sigma \\
    r & -1
  \end{pmatrix} - \lambda I\right)\\
  &= \left|\begin{matrix}
    -\sigma - \lambda & \sigma\\
    r & -1 - \lambda
  \end{matrix}\right| = (\sigma+\lambda)(1+\lambda) - r\sigma\\
  &= \lambda^2 + (1+\sigma)\lambda- (r-1)\\
  \implies \lambda_{1,2} &= \frac{-(1+\sigma) \pm \sqrt{(1+\sigma)^2 + 4(r-1)\sigma}}{2}
\end{align*}

Notice that if $r \in (0,1)$, then\footnote{As the (r-1) term is thus negative.} $(1+\sigma)^2 + 4(r-1)\sigma < (1+\sigma)^2$. Assuming \footnote{Allowing us to operate in $\mathbb{R}^n$, as this is the part under the radical.} the lhs of this inequality is greater than $0$, we can further say that $\left|\sqrt{(1+\sigma)^2 + 4(r-1)\sigma }\right|< \left|1+ \sigma\right|$. Thus, both $\lambda_1$ and $\lambda_2$ are $<0$, as taking either the positive or negative sign in the quadratic necessarily yields a negative\footnote{Based on the reasoning "above", we are essentially saying (in "pseudomath") $-\alpha + (\alpha - \epsilon) < 0$, as does $-
\alpha - (\alpha - \epsilon)$, taking $\alpha$ to represent the "terms" of the quadratic and $\epsilon$ the undetermined-but-clearly-there difference.}. We could work out a full solution, but this is unncessary; clearly, as all $\lambda_i < 0$ when $r < 1$, $(x,y,z) \to 0$ as $t \to \infty$, which supports our original assumption in simpifying the system that $(x,y,z)$ remain "small".\footnote{NB: just because the assumption "held" sts does not mean that it is \textit{always true}; it simply validates the approximation we made in the particular scenario where $r < 1$.} Thus, in all, $\underline{\mathbf{u}}(t) = \begin{pmatrix}
  x(t)\\
  y(t)\\
  z(t)
\end{pmatrix} = \sum_{j=1}^{3}\left[c_j e^{\lambda_j t}\,\underline{\mathbf{v_j}}\right] \to 0$ as $t \to \infty$, $\implies \left(\left|x(t)\right|,\left|y(t)\right|,\left|z(t)\right|\right)$ "small" $\forall t > 0$.\footnote{Note that we did not need to find the eigenvectors, or even the values (albeit, the matrix was in a quite nice form to allow this).}

\begin{figure}[!h]
  \centering
  \includegraphics*[width=0.5\textwidth]{figures/rlessthan1.png}
\end{figure}

Again, this is all under $r > 1$; when $r>1$, $4(r-1)\sigma > 0 \implies \sqrt{(1+\sigma)^2 + 4(r-1)\sigma} >0 \implies -(1+\sigma) + \sqrt{(1+\sigma)^2 + 4(r-1)\sigma} > 0$, ie the root of the characteristic polynomial when we take the positive is now greater than one. In practical terms, this indicates a positive eigenvalue (we will take this to be $\lambda_1$ and $\underline{v}_1$), and thus one of our terms will grow as $t \to \infty$; however, the other two eigenvalues remain $<1$ and will continue to shrink with time.
% TODO: fix below
% \begin{figure}[!h]
%   \centering
%   \includegraphics*[width=0.5\textwidth]{figures/rgreaterthan1.png}
% \end{figure}

Remember that, this whole time, we are working with a linearized version of the original Lorenz equations, and thus these diagrams are not fully reflective of reality. As shown, for instance, the Lorenz equations have two other steady states (unless bifurcation\dots), which influence trajectories in the original system. However, this linearization is still useful in analyzing the dynamics of the system near the steady state $(0,0,0)$.

The other two steady states influence the dynamics of the system such that, at certain initial conditions, the trajectories will tend to spiral towards one of the two steady states, as well as (in chaotic systems) jump "randomly" from one steady state to the other, hence the "attractor" name.

\subsection{Motivations of Maps}

While aforementioned dynamical systems were defined via ordinary differential equations, we can also define them via \textbf{maps}, which are discrete dynamical systems. These can be defined:
\begin{enumerate}
  \item Taking the maxima of a function in a system plotted against the previous maxima (for instance, in the Lorenz map, taking the maxima of $z(t)$ as $z_n$ and plotting $z_n$ against $z_{n+1}$ for natural $n$).
  \item "Redefining" ODE's as maps. For instance, the \textbf{logistic map} defined \[x_{n+1} = f(x_n) = \lambda x_n(1-x_n)\]
\end{enumerate}
% TODO : tent, maps, etc

\newpage
% \chapter{One-Dimensional Flows}
\part{One-Dimensional Flows}
\section{Flows on the Line}

\subsection{Introduction to Flows on the Line}

Consider $\dot{x} = f(x)$ or $\dot{x} = f(x, \mu)$ where $\mu$ some parameter. Solutions will be $x(t) \in \mathbb{R}$, and will either have $f: \mathbb{R} \to \mathbb{R} (f(x))$ or $f: \mathbb{R}^2 \to \mathbb{R} (f(x,\mu))$.

We may consider initial conditions $x(t=0) = x_0 \in \mathbb{R}$; different $x_0$ lead to different solutions. Typically, we do not plot $x(t)$ against $t$, rather, we show the dynamics in $\mathbb{R}$ on a phase plot.

We could, in principle solve ODEs exactly; eg $\dot{x} =f(x), t \geq 0, x(0) = x_0$. We have \begin{align*}
  \dv{x}{t} &= f(x)\\
  \int \frac{dx}{f(x)} &= \int dt\\
  \int_{x(0)}^{x(t)} \frac{dx}{f(x)} &= \int_0^t dt\\
  \int_{x(0)}^{x(t)} \frac{dx}{f(x)} &= t
\end{align*}
From here, we would have to solve the integral on the left and solve for $x(t)$.

However, we will approach this by determining the dynamics graphically. We do the following:

\begin{enumerate}
  \item Graph $f(x)$
  \item Draw steady states when $\dot{x} = f(x) = 0$
  \item For $f(x) \neq 0$ we have either $\dot{x} = f(x) > 0, x(t)$ increasing, or $\dot{x} = f(x) < 0, x(t)$ decreasing
\end{enumerate}

\begin{remark}
  If $f(x) = 0$ and $f'(x) < 0$, then $x$ is a \textbf{stable steady state}. If $f(x) = 0$ and $f'(x) > 0$, then $x$ is an \textbf{unstable steady state}.

  If $f(x) = f'(x) = 0$, we have a steady state which is "half-stable", eg $\dot{x} = f(x) = x^2$.
\end{remark}

\begin{example}
  $\dot{x} = f(x, \mu) = x^2 + \mu$. If $\mu = 0$, we have a "half-stable" point. 

  If $\mu > 0$, we have $f(x,\mu) \geq \mu > 0 \forall x$, and we have no steady state.

  If $\mu < 0$, we have two stable steady states (on stable, one unstable).

  There is a \textbf{bifurcation} at $x=0$ when $\mu = 0$; the number of steady states changes.
\end{example}

In short; $\text{sign}(f'(x))$ determines the stability of the steady state, given $f'(x) \neq 0$, in which case we need to study further.

In particular, cases where $f(x)=f'(x)=0$ are "delicate", and small parameter changes can cause large changes in the dynamics.

\subsection{Linear Stability Analysis}

$\dot{x} = f(x)$, let $x^{*} \in \mathbb{R}$ be a s.s., ie $f(x^*) = 0$; what does the dynamics look like near $x^*$?

First, change variables such that the s.s. is at the origin; let $v(t) = x(t) -x^*$, then 
\begin{align*}
  \dot{v} &= \dv{t}(x(t)-x^*)=\dv{x}{t}-0\\
  &= f(x(t)) = f(x^* + v(t)) = g(v(t))
\end{align*}

Note that this "new" system has a steady state at $v=0$; $g(0) = f(x^*) = 0$. Here, we can Taylor expand $\dot{v}$:
\begin{align*}
  \dot{v} &= \sum_{j=0}^\infty \frac{v^j f^{j}(x^*)}{j!}\\
  &= f(x^*) + v f'(x^*) + \frac{v^2}{2} f''(x^*) + \dots\\
  f(x^*) = 0 \implies \dot{v} &= f'(x^*)v+\frac{1}{2}f''(x^*)v^2 + O(v^3)
\end{align*}

For $x(t) \approx x^*$, we have $|v(t)| = |x(t) - x^*| \ll 1$, then $1 \gg |v(t)| \gg |v(t)|^2 \gg |v(t)|^3 \gg \dots > 0$. Provided $f'(x^*)\neq 0$ for $|v|$ sufficiently small the $f'(x^*) v$ term will dominate others and we can write \[\dot{v} \approx f(x^*)v.\]
Let $\lambda = f(x^*)$ (just a constant), then we say 
\begin{align*}
  \dot{v} &= \lambda v\\
  \implies \dv{v}{t} &= \lambda v\\
  \implies \int_{v_0}^{v(t)}\frac{dv}{v} &= \int_0^t \lambda dt\\
  \implies [\ln |v|]_{v_0}^{v(t)} &= \lambda t\\
  \implies \ln \frac{v(t)}{v_0} &= \lambda t
\end{align*}

We can drop the absolute value bars as $v(t)$ and $v_0$ must have the same sign.
% TODO: clarify
\begin{align*}
  v(t) &= v_0 e^{\lambda t}\\
\end{align*}

Thus, if $\lambda < 0$, $v(t) \to 0$ as $t \to \infty$, then $v(t) = x - x^* \implies x(t) \to x^*$ as $t \to \infty$. Else, if $\lambda > 0$, then $|v_0 e^{\lambda t}|$ grows with $t$ and $|v(t)| \to \infty$ as $t \to \infty$. Importantly, $|v(t)|$ becomes large before it becomes unbounded, meaning that our initial assumption doesn't work as the $O(v^2)$ term becomes significant. However, we can still make conclusions about the (in)stability local to $x^*$, and we can't draw conclusions for dynanmics $t \to \infty$.

\begin{example}
  $\dot{x} = f(x) = x-x^3 = x(1-x^2) = x(1-x)(1+x)$. We have steady states at $x = -1,0,1$.

  $f(x) = x - x^3 \implies f'(x) = 1 - 3x^2$, so $f(0) = 1 \implies 0$ unstable, $f'(\pm 1) = 1 -3 = -2 \implies \pm 1$ stable.

  Alternatively, graph $f(x)$ and the steady states are visually obvious.
\end{example}

This is called \textbf{linear stability analysis}, as are reducing the nonlinear differential equation $\dot{x} = f(x)$ to a linear ODE $\dot{v} = \lambda v$ (we'll see in higher dimensions that we replace $\lambda$ with some Jacobian).

\subsection{Existence \& Uniqueness}

We are studying the \textit{qualitative} behavior of solutions, which only makes sense if these solutions exist. Usually we require that the IVP $\dot{x} = f(x), x(0) = x_0$ to have a unique solution $x(t) \forall t \geq 0$, for every $x_0 \in \mathbb{R}$. If they were not unique, they have multiple solutions starting at some point or worse, at points $x_0$ where multiple solutions are possible orbits will cross each other. The very aspect that this doesn't happen is what makes phase plots useful.

This is also why only autonomous ODEs are considered; in nonautonomous ODES $\dot{x} = f(t,x)$, the value of $\dot{x}(t)$ at a particular value of $x$ will depend of $t$. For instance, consider $\dot{x} = -x+t$; without a $t$, this has very straightforward dynamics, but with the $t$ parameter, there are no fixed points and can't be analyzed as easily without solving exactly. Time-dependent problems will not be considered in this course, but they can be dealt with; esp, consider periodic forcing, eg $\dot{x} = x - x^3 + \cos t$. To study system such as this, we define a \textit{map}.

Let $T$ be the period of the forcing, $T = 2 \pi$ here. Define a map \[S_T: x_0 \mapsto x (2\pi),\] where $x(2 \pi) = x(T)$ solves the ODE with initial $x(0) = x_0$. Note that $\cos$ takes the same values in $[0, 2\pi]$ as in $[2\pi, 4\pi]$, so we can use the same map $S_T$ to map from $x(2 \pi)$ to $x(4 \pi)$.

A fixed point of the map $S_T$ is a periodic orbit of the ODE of period $T$. In forced systems, period orbits must have period $n T$, an integer multiple of the forcing period; we'll stick with $\dot{x} = f(x)$ with $f: \mathbb{R} \to \mathbb{R}$.

Lets consider $x(0) = x_0$; when is a solution $x(t)$ unique? When does it exist? What can "go wrong"/when?

\begin{example}\label{ex:bad1}
  $\dot{u} = u^{\frac{1}{3}}, u(0) = 0$. 
  
  Clearly, if $u(t) = 0 \forall t$ satisfies the ODE, and the initial condition (stays at 0). But we can also solve it exactly:
  \begin{align*}
    \dv{u}{t} = u^{\frac{1}{3}} \implies\int_{u(0)}^{u(t)} \frac{du}{u^{\frac{1}{3}}} &= \int_0^t dt\\
    \left[\frac{3}{2}u^{\frac{2}{3}}\right]_0^{u(t)} &= t\\
    u(t) &= \left(\frac{2t}{3}\right)^{\frac{3}{2}}
  \end{align*}

  This is different to previously; is it still a valid solution? Clearly, satisfies $u(0) = 0$. And for $t \geq 0$, $u(t) = (\frac{2t}{3})^{3/2} \implies \dv{u}{t} = \frac{3}{2}\left(\frac{2t}{3}\right)^{1/2}\cdot \frac{2}{3} = u^{1/3}$, and thus also satisfies ODE.

  Notice that this fails when $t < 0$, as we would have to take the square root of a negative number. Thus, the solution only valid for $t \geq 0$ and at $t = 0$ it has $u(0) = \dot{u}(0) = 0$. "Appending" our first solution, we write:
  \[u(t) = \begin{cases}
    0 & t < 0\\
    \left(\frac{2t}{3}\right)^{\frac{3}{2}} & t \geq 0
  \end{cases},\] which is defined for all $t \in \mathbb{R}$, continuous for all $t \in \mathbb{R}$, differentiable for all $t \in \mathbb{R}$, $\dot{u}(t)$ is continuous, and satisfies $u(0) = 0$.

  However; we actually have two "new" solutions; \[u(t) = \begin{cases}
    0 & t < 0\\
    +\left(\frac{2t}{3}\right)^{\frac{3}{2}} & t \geq 0
  \end{cases}\] or \[u(t) = \begin{cases}
    0 & t < 0\\
    -\left(\frac{2t}{3}\right)^{\frac{3}{2}} & t \geq 0
  \end{cases}.\]

  For any $t_0 \geq 0$, $u(t) = \begin{cases}
    0 & t \leq 0\\
    \left(\frac{2}{3}\left(t-t_0\right)\right)^{\frac{3}{2}} & t \geq t_0
  \end{cases}$, ie staying at $0$ for arbitrarily long time, then "spitting" away at $t = t_0$. Thus, this IVP has infinitely many solutions, and thus is not unique.
   
  In $\dot{u} = f(u) = u^{1/3}$, $f(u)$ is continuous on $\mathbb{R}$, and differential \emph{except} at $u = 0$, causing the non-uniqueness here; any other $u(t_0)$ would yield a unique solution.

  In phase space, $f(u)$ has an unsteady state at $0$, and is increasing for $u > 0$ and decreasing for $u < 0$.
\end{example}

\begin{example}\label{ex:bad2}
  $\dot{u} = f(u) = u^3$. Now, $f$ is continuous and differentiable on $\mathbb{R}$. We have an unsteady ss at $0$ again. Solving:
  \begin{align*}
    \dot{u} = u^3 \implies \int \frac{du}{u^3} &= \int dt\\
    -\frac{1}{2u^2} &= t+C\\
    \frac{1}{u^2} &= -2t -2c = k -2t\\
    u^2 &= \frac{1}{k-2t}\\
    u &= \pm \frac{1}{(k-2t)^\frac{1}{2}}.
  \end{align*}
  Let $u(0) = u_0 > 0$ (symmetrical over $u_0 < 0$). We need to take the positive square root for a positive $u_0$, so $u = \frac{+1}{(k-2t)^{1/2}}$. \begin{align*}
    u(0) = u_0 \implies u_0 &= \frac{1}{(k-0)^{1/2}}\\
    \cdots k & = \frac{1}{u_0^2}\\
    \implies u(t) &= \frac{1}{\left(\frac{1}{u_0^2} - 2t\right)^{1/2}}
  \end{align*}
  For $u_0 > 0$ this solution becomes unbounded in \emph{finite} time. $u(0) = u_0 > 0$, then as $t \to \frac{1}{2u_0^2}, u(t) \to + \infty$.

  "Purely" speaking, this is an issue, as the solution does not exist $\forall$ time. In "applications", we don't worry about this since if $u(t)$ getting "very large", it is already outside of the reasonable range of validity for the model.
\end{example}

\begin{theorem}
  Consider the IVP $\dot{x}= f(x), x(0) = x_0 \in \mathbb{R}$. Suppose $f(x), f'(x)$ are both continuous on an open interval $I \subset \mathbb{R}$ and $x_0 \in I$. The, $\exists T > 0$ s.t. the problem has a solution $x(t)$ for $t \in (-T, T)$ with $x(t) \in I$ for all $t \in (-T,T)$, and this solution is unique.
\end{theorem}

\begin{remark}
  Won't be proven.
\end{remark}

\begin{remark}
  For \cref{ex:bad1}, $f'(x)$ not continuous at $x = 0$, and thus the theorem does not apply. Notice that if we consider the same system with $x(0) \neq 0$, we can choose some $I$ which contains $x_0$ but not $0$ and apply it there.
\end{remark}

\begin{corollary}
  If $f(x), f'(x)$ both continuous on all $\mathbb{R}$, then the solution exists and is unique $\forall (T_-, T_+)$, and either $T_- = - \infty$ or $|x(t)| \to \infty$ as $t \to T_-$ (and similar for $T_+$).
\end{corollary}

In this course, we'll always consider $f$ as differentiable as we need it to be. $f \in C^1$ (\footnote{once continuously differentiable})then solutions are unique while bounded; this is often sufficient, but we'll need more differentiability for bifurcation theory, ie often take infinitely differentiable functions.

\subsection{Impossibility of Oscillations}

Suppose $\dot{x} = f(x)$ has a periodic solution so $x(t+T) = x(t) \forall t \in \mathbb{R}$ for some period $T > 0$.

For a given value of $x$, $\dot{x}$ changes sign each time solution crosses $x$, so $f(x)$ also changes sign. But $f(x)$ uniquely defined for any particular value of $x$, so this cannot happen, ie, $\dot{x} = f(x)$ \textit{cannot} have periodic solutions.

The same argument shows that all solutions $x(t)$ of $\dot{x} = f(x)$ for $x \in \mathbb{R}$ are \textbf{monotonic}.

\subsection{Potential or Gradient Flows}

Idea: potential flow, dynamics evolve only "down hill". Stable steady states occur when $V(u)$ is at a minimum, and unstable steady states occur when $V(u)$ a maximum. 

Mathematically, we just have $\dot{u} = f(u) = - \dv{V}{u}$. Consider the evolution of $V(u(t))$ as a function of $t$ on a solution:

\begin{align*}
  \dv{t} \left[V(u(t))\right] &= \dv{V}{u} \cdot \dv{u}{t}\\
  &= - \left(\dv{V}{u}\right)^2
\end{align*}
So $\dv{t} \left[V(u(t))\right]$ strictly negative unless $\dv{V}{u} = 0$ when $V$ is flat and $\dot{u} =0$.

Note that any 1 dim. dynamical system can be thought of as a potential flow. Say $\dot{u} = f(u)$; for a potential flow, we need $f(u) = - \dv{V}{u}$, so let $V(u) = - \int f(u) \dd{u}$.

\begin{definition}[Double Well Potential]
  Defined by \begin{align*}
    V(u) &= -\frac{1}{2}u^2 + \frac{1}{4}u^4\\
    \implies \dot{u} &= u - u^3
  \end{align*}
\end{definition}

In higher dimensions, we may consider \textbf{gradient} flows. Consider $\dot{u} = f(u), u \in \mathbb{R}^d$ with $f(u) = - \grad V(u)$ where $V: \mathbb{R}^d \to \mathbb{R}$.

\section{Bifurcation Theory}

\subsection{Implicit Function Theorem}

A solution $u(t) \in \mathbb{R}$ for an ODE but suppose the ODE depends on a parameter $\mu$ and so $\dot{u} = f(u, \mu), u(t), \mu \in \mathbb{R}$. We always keep $\mu$ \textit{fixed} when we solve the ODE; we are interested in how this change when we resolve  with different $\mu$.

Take $f(u,\mu) : \mathbb{R}^2 \to \mathbb{R}$, and we'll always assume $f$ is continuous of each variable and any "necessary" derivatives exist. For "boring cases", we can use the implicit function theorem.

\begin{theorem}[Implicit Function Theorem]\label{thm:ift}
  If $f(u, \mu)$ is a continuously differentiable function of $u$ and $\mu$, and $\exists (u^*,\mu^*) \in \mathbb{R}^2$ s.t. $f(u^*,\mu^*) = 0$\footnotemark and $\pdv{f}{u}(u^*,\mu^*) \neq 0$, the $\exists$ a neighborhood of $(u^*, \mu^*) \in \mathbb{R}^2$ and a continuously differentiable function $g: \mathbb{R} \to \mathbb{R}$ s.t. $g(\mu^*)= u^*$ and for $\mu$ close to $\mu^*$ we have \[f(g(\mu), \mu) = 0\] and, moreover, for each such $\mu$ close to $\mu^*$, $g(\mu)$ is a unique value of $u$ close to $u^*$ s.t. $f(u, \mu) = 0$.\footnotemark
\end{theorem}

\footnotetext{$f(u^*, \mu^*) = 0 \implies \mu = u^*$ there is a s.s. at $u^*$}
\footnotetext{Where a "neighborhood" refers to an open set containing the point (as long as point not on boundary).}

Note that the theorem states that $f(u, \mu) \neq 0$ at all points in the neighborhood \textit{not} on the curve $u = g(\mu)$.

In short, the theorem is stating that $u^*$ is a ss when $\mu = \mu^*$ and $\pdv{f}{u}(u^*, \mu^*)\neq 0$, then if we "change $\mu$ slightly", then $\mu^*$ will no longer be a ss, but at $\mu = g(\mu)$ \textit{close} to $\mu^*$, there is a ss. So, the steady state gets perturbed slightly when $\mu$ changes, and moreover, the assumption $\pdv{f}{u}(u^*, \mu^*)\neq 0$ means we have cases where linearization determines stability.

\begin{align*}
  \pdv{f}{u}(u^*, \mu^*) > 0 \implies u^* is unstable\\
  \pdv{f}{u}(u^*, \mu^*) < 0 \implies u^* is stable
\end{align*}

For small enough perturbation $\mu$ from $\mu^*$, $\pdv{f}{u}(g(\mu), \mu)$ will have the same sign as $\pdv{f}{u}(u^*, \mu^*)$ so the stability of the perturbed fixed point will also have the same stability as the original fixed point.

\begin{remark}
  Since $0 = f(g(\mu), \mu)$, differentiate using chain rule:
  % TODO; verify
  \begin{align*}
    0 &= \dv{\mu} f(g(\mu), \mu)\\
    &= g'(\mu)\pdv{f}{u} \left(g(\mu), \mu\right) + \pdv{f}{\mu} \left(g(\mu), \mu\right)\\
    \implies g'(\mu) &= - \frac{\pdv*{f}{\mu} \left(g(\mu), \mu\right)}{\pdv*{f}{u} \left(g(\mu), \mu\right)}
  \end{align*}
  Since $\pdv{f}{u} \left(g(\mu^*), \mu^*\right) \neq 0$, we have \[g'(\mu^*) = \frac{-\pdv{f}{\mu} \left(g(\mu^*), \mu^*\right)}{\pdv{f}{u} \left(g(\mu^*), \mu^*\right)}\]. 

  We (could) similarly find $g''(\mu^*)$.
\end{remark}

\begin{remark}
  \cref{thm:ift} is only defined on a neighborhood because its possible for $\pdv{f}{u} (g(\mu), \mu) = 0$ or $u = g(\mu)$ can break down if we try to push the result "too far" from the initial point.
\end{remark}

\begin{example}
  Let $\dot{u} = f(u, \mu) = u - \mu$. If $\mu = \mu^* = 0$, then $\dot{u} = u \implies \dot{u} = 0$ is a ss; no matter how we vary $\mu$, we have identical dynamics, with just the exact location of the ss varying, and there are no qualitative changes with varying $\mu$. This is because $f(u, \mu) = u - \mu \implies \pdv{f}{u} \left(u, \mu\right) = 1 - 0 > 0$. To obtain qualitative changes in dynamics, we need \cref{thm:ift} to \emph{not} apply (ie $f(u^*, \mu^*) = 0$ \& $\pdv{f}{u} \left(u^*, \mu^*\right) = 0$, equiv. $f(u^*, \mu^*) = f_u (u^*, \mu^*) = 0$).
\end{example}

\subsection{Saddle-Node/Fold Bifurcation}

\begin{example}
  Consider $\dot{u} = f(u, \mu) = \mu + u^2$. Notice that $f(0,0) = 0$ so $u = 0$ is a ss when $\mu = 0$. $f_u(u, \mu) = 2u =0$ for $u = 0$, so \cref{thm:ift} does not apply. We can solve directly for the dynamics.

 When $\mu < 0$ we have two steady states; as $\mu$ approaches 0, these combine and we have a half-stable steady state and $\mu = 0$, and finally when $\mu > 0$ we have no steady states.

 Notice that varying $\mu$ without changing its sign does not change the dynamics qualitatively.

 We can also show this algebraically; $0 = \mu + u^2 \implies u = \pm \sqrt{-\mu}, \mu < 0$, and $u = 0$ when $\mu = 0$.

 $f_u = 2u$, so $f_u(+\sqrt{-\mu}, \mu) = 2 \sqrt{-\mu} > 0$ and $f_u(-\sqrt{-\mu}, \mu) = -2 \sqrt{-\mu} < 0$. So, the positive root is unstable and the negative is stable. The bifurcation will occur when $f(\pm \sqrt{-\mu}m \mu) = 0 \implies u = \mu = 0$.
\end{example}

\begin{definition}[Bifurcation Point]
  Any point $(u^*, \mu^*) \in \mathbb{R}^2$ where qualitative dynamics change is called a \emph{bifurcation point}.

  \textbf{NB:} a bifurcation point requires \emph{both} a particular $\mu$ and $u$.
\end{definition}

In dynamical system, it is useful to foliate the phase space and plot phase portraits for the whole family of dynamical systems together (ie, plot "against" $\mu$). This is called a \textit{bifurcation diagram}. Solid lines represent stable steady states, and dashed lines represent unstable steady states.

% TODO: add

In our example, we have what is called a \textbf{fold bifurcation}, named as such for the shape that occurs in the bifurcation graph.

Note that $\dot{u} = -\mu + u^2$, $\dot{u} = - \mu - u^2$, and $\dot{u} = \mu - u^2$ are share similar bifurcation diagrams, but with different dynamics.

\begin{example}
  $\dot{u} = f(u, \mu) = \mu - u - e^{-u}$. Solving this exactly is either hard or impossible (and in any case annoying), but no need.

  Steady states are defined by $0 = f(u, \mu) = \mu - u - e^{-u}$. This can be solved with the Lambert-W function (but don't bother).

  We can write $\mu = u+e^{-u}$, then plot $\mu$ as a function of $u$ and swap the axes to see the steady states $u$ for each $\mu$.

  Finally, we can approach this graphically. Let $g_1(u, \mu) = \mu - u, g_2(u, \mu) = e^{-u}$, then $f(u,\mu) = g_1(u,\mu) - g_2(u,\mu)$ so steady states occur when $g_1(u, \mu) = g_2(u,\mu)$.

  % TODO graph
  Graphically, we have either no, one, or two steady states depending on $\mu$.

  For $\mu \ll 0$ no s.s.; for $\mu \gg 0$ two; we infer that there is one s.s at some $\mu$, which will be at the fold bifurcation. Call $u, \mu$ at this point $(u^*, \mu^*)$.

  If $\mu > \mu^*$:
  % TODO graph
  For $u$ between steady states, $g_1 > g_2$ so $f(u,\mu) > 0$. For $u \gg 0$ or $u \ll 0$, then $g_2>g_1$ and $f<0$. Thus, we have the 
  following dynamics:

  If $\mu < \mu^*$, $f < 0$;

  and for $\mu = \mu^*$:
  % TODO graphs

  All together, we have the following bifurcation diagram:
  % TODO graph

  To find the bifurcation point, there are two approaches; either from the graphs, solve for a point $(u^*, \mu^*)$ where $g_1, g_2$ touch. Think of $g_1, g_2$ as functions of $u$ with $\mu$ as a parameters;

  \begin{align*}
    g_1 = \mu - u &\implies \pdv{g_1}{u} = -1\\
    g_2 = e^{-1} &\implies \pdv{g_2}{u} = \pdv{g_1}{u} = -1\\
   & \implies - e^{-u} = -1\\
    &\implies \log (e^{-u}) = log (1) = 0 \implies u = 0
  \end{align*}

  But $g_1(u, \mu)  = \mu$ when $u = 0$, and $g_2(u, \mu) = e^{-1} = 1$ when $u = 0$, so for the graphs to touch, $\mu = 1$. Then, $g_1, g_2$ have the same values and slope when $u = 0$. The bifurcation point is $(u^*, \mu^*) = (0,1)$.

  Alternatively, at bifn point, we have that $f(u^*, \mu^*) = f_u(u^*, \mu^*) = 0$, so $0 = \mu^{*} - u^{*} - e^{-u^*}$ and $0 = f_u = -1 + e^{-u^*}$, which give the exact same equations (and, naturally, answers) as above.

  Note: We used $g_1 > g_2$ or $g_1 < g_2$ to find the stability; alternatively, we can differentiate $f$ wrt $u$, and find $f_u = -1 + e^{-u}$.

  If $u > 0$, $e^{-u} < 1$ and $f_u(u, \mu) < 0$, so we have stable ss. If $u < 0, e^{-u} > 0$ and $f_u(u, \mu) > 0$, so we have unstable ss. Then, we again have the bifurcation at $u = 0$.
\end{example}

\begin{remark}
  Notice that this example's bifurcation diagram is very similar to the last; notice that \begin{align*}
    \dot{u} &= \mu - u - e^{-u}\\
&= \mu - u -[1-u+\frac{1}{2}u^2 - \frac{1}{6}u^3 + \dots]\\
&= (\mu - 1) - \frac{1}{2}u^2 + \frac{1}{6}u^3 - \dots
  \end{align*}
  Let $w = \mu - 1$, then $$\dot{u} = w - \frac{1}{2}u^2 + \frac{1}{6}u^3 - \dots$$. Let $v = \frac{u}{2}$; then $$\dot{v} = \frac{\dot{u}}{2} = \frac{1}{2}\left(w - \frac{1}{2}u^2 + \frac{1}{6}u^3 + \dots \right).$$
  Let $\lambda = \frac{w}{2}$ then $\lambda = \frac{1}{2}(\lambda - 1)$. Then $$\dot{v} = \underbrace{\lambda - v^2}_{\text{as in first example}} + \underbrace{\frac{2}{3}v^3 + O(v^4)}_{\text{very small when $v \approx 0$}}$$
\end{remark}
\begin{remark}[aside]
  There exist so-called "near identity transformations" to remove these additional terms. Let $\sim v = v + $(something small), so $\dot{\tilde{v}}= \lambda - \tilde{v}^2 + O(\tilde{v}^4)$, then we can make the higher order terms "as high" as we want (again, this is all "valid" near the bifurcation point).

  Overall, we can explain the similarities in the past two examples by changing variables effectively.
\end{remark}

To recognize saddle-node/fold bifurcations in a system, either:

\begin{itemize}
  \item observe a change in the number/stability of steady states; % TODO graphs
  \item observe something like $\dot{u} = \mu - u^2 - O(u^3)$ in a differential equation;
  \item check rigorous conditions. There are four conditions.
\end{itemize}

\begin{definition}[Saddle-Node Conditions]
  The following must hold for a saddle-node bifurcation to exist at $(u^*, \mu^*)$:
  \begin{enumerate}[label=\roman*)]
    \item $f(u^*, \mu^*) = 0$
    \item $f_u(u^*, \mu^*) = 0$
    \item $f_{uu}(u^*, \mu^*) \neq 0$
    \item $f_\mu(u^*, \mu^*) \neq 0$
  \end{enumerate}  
\end{definition}

\begin{remark}
  $\dot{u} = \mu - u^2$ is called the \textbf{normal form} for the saddle-node bifurcation. Other examples of fold bfn can be rewritten as a perturbation of the normal form by change of variables (as shown above).
\end{remark}

If one or the other of condition (iii), (iv) fail, then we have a different bifurcation.

\subsection{Transcritical Bifurcations}

Consider $\dot{u} = f(u,\mu) = \mu u - u^2$; this is the normal form of a \textbf{transcritical bifurcation}.

Notice that $f(0,0) = 0$, and $f_u(0,0) = 0$, so we expect a bifurcation at $(0,0)$. $f_{uu}(u, \mu) = -2 \neq 0$, and $f_{\mu}(\mu, u) = u \implies f_{\mu}(0,0) = 0$. So, (iv) of the saddle-node conditions fails.

By inspection, we have a steady state for $u = 0$ or $u = \mu$. So we have two steady states, unless $\mu = 0$, then they collide at $u = 0$.

\subsubsection{Analyzing Stability}
$f_u(u,\mu) = \mu - 2u$, so when $u = 0$, $f_u = \mu$. $f_u = \mu > 0$ for $\mu > 0$, so unstable, and $f_u < 0$ for $\mu < 0$, so $u = 0$ stable.

For $u = \mu$, $f_u(\mu,\mu) = \mu - 2 \mu = -\mu$, so steady state $u = \mu$ has opposite stability of the s.s.s at $u=0$. So, we have the following bifurcation diagram:
% TODO graph


\begin{example}
  $\dot{x} = f(x) = x(1-x^2) - a(1-e^{-bx})$. $e^0 = 1 \implies f(0) = 0 \forall t$, so $x=0$ is a steady state for all parameters.

  We first check stability of this steady state:
  $f_x(x) = (1-x^2)-2x^2-abe^{-bx} = 1-3x^2 - abe^{-bx}$. $f_x(0) = 1 - ab$, so for $ab < 1 \implies f_x > 0 \implies x = 0$ unstable, and for $ab > 1 \implies f_x < 0 \implies x= 0$ stable. Thus, we can expect a bifurcation when $ab=1$, as the stability would change here.

  Since we are interested in $x\approx 0$, lets expand:
  $$e^{-bx} = 1 - bx + \frac{1}{2}b^2x^2 - \cdots,$$then 
  \begin{align*}
  \dot{x} &= x(1-x^2)-a(1-[1-bx+\frac{1}{2}b^2x^2+\cdots])\\
  &= \underbrace{(1-ab)}_{\mu}x+\frac{1}{2}ab^2x^2 +\underbrace{O(x^3)}_{\text{contains factor of } x^3}\\
  &\approx \mu x + (\frac{1}{2}ab^2)x^2
  \end{align*}
  which is very similar to the normal form of the trans-critical bifurcation.

  Since the bifurcation occurs at $ab = 1$, then $\frac{ab^2}{2}\approx \frac{b}{2}$ near bifn, and so \[\dot{x} \approx x [\mu + \frac{b}{2}x] + O(x^3).\] For $b < 0$, similar to previous example.

  The extra fixed ss is then given by $\mu + \frac{b}{2}x \approx 0 \implies x \approx -\frac{2\mu}{b} = \frac{-2(1-ab)}{b}$.
\end{example}

\begin{definition}[Conditions for Transcritical]
  Let $\dot{u} = f(u,\mu)$. The following are required for a transcritical bifurcation:
  \begin{enumerate}[label=\roman*)]
    \item $f(0,0) = 0$ (steady state)
    \item $f_u(0,0) = 0$ (bifurcation exists)
    \item $f_{uu}(0,0) \neq 0$
    \item $f_\mu(0,0) = 0$ (not a fold bifurcation)
    \item $[f_{u\mu}(0,0)]^2 > f_{uu}(0,0)f_{\mu \mu}(0,0)$
  \end{enumerate}
\end{definition}

\begin{remark}
  (iv) required, otherwise we would have a fold bifurcation. (v) insures a transcritical bifurcation. In the normal form, $\dot{u} = f(u, \mu) = \mu u - u^2$, we have $f_{\mu \mu} = 0$, and so for (v) to hold, everything simplifies to require that $f_{u, \mu} \neq 0$.
\end{remark}


\subsection{Pitchfork Bifurcations}

There are two types of pitchfork bifurcartions.

\subsubsection{Supercritical Pitchfork}
Normal form:
$$\dot{u} = \mu u - u^3 = f(u, \mu)$$
Notice that $f$ is an odd function of $u$. These bifurcations are often seen with this type of (anti)symmetry, thought not exclusively.\\
$u = 0$ is a ss $\forall \mu \in \mathbb{R}$. For stability,
\begin{align*}
  f_u &= \mu - 3u^2\\
  &\implies f_u(0, \mu) = \mu,
\end{align*}
so $u=0$ stable for $\mu < 0$, unstable for $\mu > 0$.\\
However, $f_{uu} = - 6u \implies f_{uu}(0,0) = 0$, so \textit{not} a transcritical bifurcartion.\\
Solving algebraically:
\[0 = f(u, \mu) = u[\mu - u^2]\] so $u = 0$, or $u^2 = \mu \implies u = \pm \sqrt{\mu}$ when $\mu > 0$. Thus, new steady states are "born" at $u = 0$ when $\mu =0$, so this is the bifn. \\For stability,
\[f_u = (\pm \sqrt{\mu}, \mu) = \mu - 3(\pm \sqrt{\mu})^2 = -2 \mu,\] so both $u = +\sqrt{\mu}$ and $u = - \sqrt{\mu}$ have opposite stability to $u = 0$, when they exist.\\

\subsubsection{Subcritical Pitchfork}

$$\dot{u} = f(u,\mu) = \mu u + u^3.$$ As before, $f(0, \mu) = 0 \forall \mu \in \mathbb{R}$, so $u = 0$ always ss. For stability \[f_u = \mu + 3u^2,\] and $f_u(0, \mu) = \mu$ and $u  0$ stable when $\mu <0$ and unstable when $\mu > 0$.\\
For $\cancel{0}$ ss, solve exactly
\begin{align*}
  0 = f(u, \mu) = \mu u + u^3 &= u(\mu + u^2)\\
  \implies u^2 = -\mu, &\implies u = \pm \sqrt{- \mu}, \mu < 0
\end{align*}
For stability,
$$f_u(+\sqrt{-\mu}, \mu) = \mu + 3(-\mu) =-2\mu > 0\, (\text{since } \mu <0)$$
This is unstable, and similarly is $- \sqrt{-\mu}$. We essentially have an inverted pitchfork compared to the previous.\\ 
This is called \textit{subcritical} because when the persistent steady state is unstable, there is no nearby stable steady state, \textit{not} because of the direction of the pitchfork.

\begin{definition}[Conditions for Pitchfork Bifurcations]
  Normal form, $\dot{u} = f(u, \mu) = \mu u \pm u^3$.
  \begin{enumerate}[label=\roman*)]
    \item $f(u^*, \mu^*) = 0$
    \item $f_u(u^*, \mu^*) = 0$
    \item $f_{uu}(u^*, \mu^*) = 0$
    \item $f_\mu(u^*, \mu^*) = 0$
    \item $f_{uuu}(u^*, \mu^*) \neq 0$
    \item $f_{u\mu}(u^*, \mu^*) \neq 0$
  \end{enumerate}
  Other conditions exist to determine whether sub/super. Not practical to use.
\end{definition}


\begin{example}[3.4.1]
  $\dot{u} = u - \beta \tanh u = f(u, \beta)$\footnotemark\\
  $\tanh(0) = 0 \implies f(0, \beta) = 0 \forall \beta, u=0$ always a ss. Stability; $f_u(u,\beta) = 1 - \beta \sech^2u \implies f_u(0,\beta) = 1 -\beta \sech^2(0) = 1 - \beta.$\\$\beta > 1 \implies f_u(0, \beta)<0 \implies u = 0$ stable.\\
  $\beta< 1\implies f_u(0, \beta) > 0 \implies u = 0$ unstable.\\
  This indicates to existence of a bfn at $\beta = 1$. To find what type, we need to solve $u = \beta \tanh u$.\\
  Let $g_1 = u, g_2 = \beta \tanh u \implies 0 = f(u, \beta) = g_1 - g_2 \implies g_1 = g_2$ at ss.\\
  If $\beta < 0$, $u = 0$ is the only steady state, and we have that $f=g_1-g_2 = \begin{cases}
    >0 & u>0\\
    <0 &u < 0
  \end{cases} \implies u = 0$ unstable.\\
  If $\beta\gg 1$ there will be three intersections of $g_1, g_2$. For $\beta \gg 0$, $g_1 = u$ intersects $g_2 = \beta \tanh u$ on part of the graph where the function is very flat, ie near $\tanh u \approx \pm 1$ so the extra fixed points are near $u =\pm \beta$.\\
  We know that $u = 0$ changes stability when $\beta = 1$, so a bfn will occur when $u = 0 $ and $\beta = 1$. $g'(u) = 1$ and $g_2'(0) = \beta$, so when $\beta = 1$ $g_2,g_2$ intersect tangentially at $u = 0$.
\end{example}

\footnotetext{See book, pg 58}

\begin{remark}
  Its easy to plot exact solutions for steady states on a computer.\\
  Solve for $\beta = \frac{u}{\tanh u}$.
\end{remark}

\subsection{Normal Forms (Summary)}

\begin{itemize}
  \item $\dot{u} = \mu \pm u^2:$ saddle-node
  \item $\dot{u} = \mu u - u^2:$ transcritical
  \item $\dot{u} = \mu u \pm u^3:$ pitchfork
\end{itemize}

For other, non-normal forms, we can make a change of variables so that near the bifurcation point the dynamical system looks like one of these. These normal forms are all polynomials; Taylor Series.\\
Consider $\dot{u} = f(u,\mu)$ with a bifurcation at $(u^*,\mu^*)$ so $$f(u^*,\mu^*) = f_u(u^*,\mu^*) = 0.$$ By Taylor's Theorem, SS close to bfn satisfy\begin{align*}
  0 &= f(u,\mu) = f(u^*,\mu^*) + (u-u^*)f_u(u^*,\mu^*) \\&+ (\mu - \mu^*)f_\mu(u^*,\mu^*) + \frac{1}{2}(u - u^*)^2f_{uu}(u^*,\mu^*) \\&+ \frac{1}{2}(\mu - \mu^*)^2f_{\mu\mu}(u^*,\mu^*) + (u - u^*)(\mu-\mu^*)f_{\mu u}(u^*,\mu^*) + \text{h.o.t.}
\end{align*}

(or just use Taylor series in one variable multiple times). Since $f(u^*,\mu^*) = f_u(u^*,\mu^*) = 0$, we can rewrite (using Greek letters for constants:)
\begin{align*}
  0 &= \alpha (\mu - \mu^*)+\beta(u-\mu^*)^2 + \gamma (u - u^*)(\mu - \mu^*)+\delta (\mu-\mu^*)^2 + \text{ h.o.t.}
\end{align*}

This defines the steady states; letting $\dot{u} = f(u,\mu) = $ RHS, different combinations of $\alpha, \beta, \gamma, \delta$ above yield the standard bifns above.  


\subsection{"Real Examples"}

In the normal forms, there exactly one bifurcation at $(u^*,\mu^*) = (0,0)$. Often, there are multiple bfns occurring at different values of $u, \mu$.

\begin{example}
  Let $\dot{u} = \mu u - u^3 + u^5$. \\
  $u = 0$ is always a ss. We expect a pitchfork bifurcation at $(0,0)$ because $|u^5| \ll |u^3| \ll |u|$ when $u \approx 0$, so we are "near" the normal form.\\
  Solving $0 = u(\mu - u^2 + u^4)$. Additional steady states will satisfy $\mu = u^2 - u^4$. We have that $f_u = \mu - 3u^2 + 5u^4 \implies f_u (0,\mu) = \mu \begin{cases}
    >0 & \mu > 0, \text{ stable}\\
    <0 & \mu < 0 \text{ unstable}
  \end{cases}$
\end{example}

\subsection{Imperfect Bifurcations}

\begin{example}
  Consider $\dot{x} = h + rx - x^3$ with two parameters $h$ and $r$. If $h=0 \implies \dot{x} = rx-x^3$, and thus we have a pitchfork bfn at $(x,r) = (0,0)$ if $r$ is varied with $h=0$ fixed.\\
  Else, $h \neq 0$, $f(x)$ no longer symmetrical (stops begin odd) bfn changes. $h$ is often called the \emph{imperfection parameter}; harder to analyze analytically, but possible graphically.\\
  Take $g_1(x) = rx - x^3, g_2(x) = -h, \dot{r} = g_1(x) - g_2(x) \implies \text{ss at } g_1 = g_2$\\
  $r<0$, $r$ or $h$ varied s.t. $r<0$ always holds, there is a unique steady state, varies with parameters, and thus no bfns.\\
  $r>0$, there can be either 1,2, or 3 ss; fixed $r$ and changing $h$, we don't see a pitchfork; the case of two steady states will occur between 1 and 3, and we thus only have a pitchfork when $h=0$; we see a saddle-node. This occurs when $g_1$ is at a min/max, ie $g_1' = r - 3x^2 \implies x = \pm \sqrt{\frac{r}{3}} \implies g_1(+) = \frac{r^{3/2}}{3^{1/2}}(1 - 1/3) = \frac{2r^{3/2}}{3^{3/2}}$, so $h = \frac{-2r}{3}\sqrt{\frac{r}{3}}$ at a bfn. Similar behavior occurs for negative root.
\end{example}

\begin{definition}[Cusp Point]
  Point where the two curves of saddle-node bifurcations come together; an example of a co-dimension two bfn.
\end{definition}

In the earlier example; $h>h_c, h < -h_c$ implies a unique, stable ss. For $h \in (-h_c, h_c)$, there are three steady states, two of which are stable (\emph{bistable}).


\begin{definition}[Hysteresis]
  Cycle of "tipping points" due to variation of parameters.
\end{definition}

\section{Flows on Circle}
\subsection{Summary}

So far we've only consider dynamics on the real line. We can define the phase space as $\theta \in [0, 2 \pi)$, with an ODE of the form $\dot{\theta} = f(\theta)$ with periodic orbit.

\section{Two-Dimensional Dynamical Systems}
\subsection{Introduction}
A two-dimensional system has the form 
\begin{align*}
  \dot{x} = f(x,y) \text{ or } \dot{x} = f(x,y;\mu)\\
  \dot{y} = g(x,y) \text{ or }\dot{y} = g(x,y;\mu)
\end{align*}

where $\mu$ is a parameter. Let $u(t) = \begin{pmatrix}
  x(t)\\
  y(t)
\end{pmatrix} \in \mathbb{R}^2$ be a soln for some initial $u(0) = \begin{pmatrix}
  x(0)\\
  y(0)
\end{pmatrix}$.\\
Notice that $\dot(u)(t) = \begin{pmatrix}
  \dot{x}\\
  \dot{y}
\end{pmatrix} = \begin{pmatrix}
  f(x,y)\\
  g(x,y)
\end{pmatrix}$. Let $E(u) = \begin{pmatrix}
  f(x,y)\\
  g(x,y)
\end{pmatrix},$ so $\dot{u}(t) = E(u(t))$.

\begin{example}
  Nonlinear damped pendulum; $\ddot{x} + x^2 \dot{x} + \sin x = 0$.\\
  Let $\dot{x} = y$, so $\dot{y} = \ddot{x} = -x^2 \dot{x} - \sin x = -x^2y-\sin x$.
\end{example}






\end{document}