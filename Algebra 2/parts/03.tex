\begin{definition}[A More General Definition of Linear Combination]
    For a (possibly infinite) set $S$ of vectors from $V$, a \emph{linear combination} of vectors in $S$ is a linear combination of $a_1 v_1 + \cdots a_n v_n$ for some finite subset $\{v_1, \dots, v_n\} \subseteq S$.\footnotemark
\end{definition}
\footnotetext{That is, we do not allow infinite sums.}

\begin{definition}[Span]
    For a subset $S \subseteq V$, we define its \emph{span} as \[
    \Span(S):= \text{ set of all linear combinations of } S := \{a_1 v_1 + \cdots a_n v_n : a_i \in \field, v_i \in S\}.    
    \]
    By convention, we set $\Span(\varnothing) = \{0_V\}$.
\end{definition}
\begin{example}
    Let $S := \{(1, 0, -1), (0, 1, -1), (1, 1, -2)\} \subseteq \mathbb{R}^3$. Then, \[
    0_{\mathbb{R}^3} =(0,0,0) = 1 \cdot (1, 0, -1) + 1 \cdot (0, 1, -1) + -1 \cdot (1, 1, -2).
    \]
    We claim, moreover, that $\Span (S) = U := \{(x, y, z) \in \mathbb{R}^3 : x+y+z = 0\}$ (a plane through the origin).
    \begin{proof}
        Note that $S \subseteq U$, hence $S \subseteq \Span{S} \subseteq U$. OTOH, if $(x, y, z) \in U, $ we have $z = - x - y$, and so \[
        (x, y, z) = (x, y, -x - y) = x\cdot (1, 0, -1) + y \cdot (0, 1, -1) \in \Span (S)    
        \]
        hence $U \subseteq \Span (S)$ and thus $\Span (S) = U$.
    \end{proof}
\end{example}

\begin{remark}
    We implicitly used the following claim in the proof above; we prove it more generally.
\end{remark}
\begin{proposition}
    Let $V$ be a vector space over $\field$ and let $S \subseteq V$. Then, $\Span (S)$ is always a subspace. Moreover, it is the smallest (minimal) subspace containing $S$ (that is, for any subspace $U \supseteq S$, we have that $U \supseteq \Span S$).
\end{proposition}

\begin{proof}
    Because adding/scalar multiplying linear combinations of elements of $S$ again results in a linear combination of elements of $S$, and $0_V \in \Span (S)$ by definition, we have that $\Span (S)$ is indeed a subspace.

    If $U \supset S$ is a subspace of $V$ containing $S$, then by definition $U$ is closed under addition, that is, taking linear combinations of its elements (in particular, of elements of $S$); hence, $U \supset \Span (S)$.
\end{proof}

\begin{lemma}
    For $S \subseteq V$ and $v \in V$, $v \in \Span (S) \iff \Span (S \cup \{v\}) = \Span (S)$.
\end{lemma}
\begin{proof}
    ($\implies$) Let $v \in \Span (S) \implies v = a_1 v_1 + \cdots a_n v_n, a_i \in \field, v_i \in V$. Then, for any linear combination \[
    b_1 u_1 + \cdots b_m u_m + b \cdot v  = b_1 u_1 + \cdots b_m u_m + b(a_1 v_1 + \cdots + a_n v_n)   
    \]
    is a linear combination of vectors in $S \cup \{v\}$ (first equality) or equivalently, a combination of vectors in $S$ (second equality) and thus $\Span (S \cup \{v\}) \subseteq \Span{S}$. The reverse inclusion follows trivially.

    ($\impliedby$) $\Span (S \cup \{v\}) = \Span S \implies v \in \Span (S)$.
\end{proof}

\begin{example}
    (From the above example) We have \[
    \Span (\{(1, 0, -1), (0, 1, -1)\} \cup \{(1, 1, -2)\}) = \Span( \{(1, 0, -1), (0, 1, -1) \}),
    \]
    since $(1, 1, -2) \in \Span(\{(1, 0, -1), (0, 1, -1) \})$ (it was redundant, as it could be generated by the other two vectors).
\end{example}

\begin{definition}[Spanning Set]
    \defvectorspace . We call $S \subseteq V$ a \emph{spanning set} for $V$ if $\Span (S) = V$. We call such a spanning set \emph{minimal} if no proper subset of $S$ is a spanning set ($\not \exists v \in S \st S \setminus \{v\}$ spanning).
\end{definition}

\begin{remark}
    Note that any $S \subseteq V$ is spanning for $\Span (S)$. But, $S$ may not be minimal; indeed, consider the previous example. We were able to remove a vector from $S$ while having the same span.
\end{remark}

\begin{example}
    For $\field^n$ as a vector space over $\field$, the \emph{standard spanning set} $$\St_n := \{\underbrace{(1, \dots, 0)}_{:=e_1}, \underbrace{(0, 1, 0, \dots, 0)}_{:=e_2}, \dots, \underbrace{(0, \dots, 1)}_{e_n}\}.$$
    Given any $x := (x_1, \dots, x_n) \in \field^n$, we can write \[
        x = x_1 \cdot e_1 + \cdots x_n \cdot e_n.
    \]
    This is clearly minimal; removing any $e_i$ would then result in a $0$ in the $i$th "coordinate" of a vector, hence $\St \setminus\{e_i\}$ would span only vectors whose $i$th coordinate is $0$.
\end{example}

\begin{definition}[Linear Dependence]
    \defvectorspace . A set $S \subseteq V$ is said to be \emph{linearly dependent} if there is a nontrivial linear combination of vectors in $S$ that is equal to $0_V$.

    Conversely, $S$ is called \emph{linearly independent} if there is no nontrivial linear combination of vectors in $S$ that is equal to $0_V$; all linear combinations of vectors in $S$ that equal $0_V$ are trivial.
\end{definition}
